<!DOCTYPE html>
<html lang="en"><head>
<script src="14-Clustering_files/libs/quarto-html/tabby.min.js"></script>
<script src="14-Clustering_files/libs/quarto-html/popper.min.js"></script>
<script src="14-Clustering_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="14-Clustering_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="14-Clustering_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="14-Clustering_files/libs/quarto-html/quarto-syntax-highlighting-cdaacfc258cb6f151192107f105ac881.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.9.12">

  <meta name="author" content="Matteo Francia   DISI — University of Bologna   m.francia@unibo.it">
  <title>Machine Learning and Data Mining (Module 2)</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="14-Clustering_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="14-Clustering_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="14-Clustering_files/libs/revealjs/dist/theme/quarto-b3aa9dda08c8fde6dffd4aadb76df7d0.css">
  <link href="14-Clustering_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="14-Clustering_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="14-Clustering_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="14-Clustering_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning and Data Mining (Module 2)</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Matteo Francia <br> DISI — University of Bologna <br> m.francia@unibo.it 
</div>
</div>
</div>

</section>
<section id="data-mining" class="title-slide slide level1 center">
<h1>Data Mining</h1>

</section>

<section id="clustering" class="title-slide slide level1 center">
<h1>CLUSTERING</h1>

<img data-src="img/clustering/14-Clustering_0.png" class="r-stretch"><p><strong>Matteo Golfarelli</strong></p>
<p>Inter-cluster distances are maximized</p>
<p>Intra-cluster distances are minimized</p>
</section>

<section id="what-is-clustering-analysis" class="title-slide slide level1 center">
<h1>What is Clustering analysis?</h1>
<p>Finding groups of objects such that objects that belong to the same group are more “similar” to each other than objects belonging to different groups</p>
</section>

<section id="what-is-not-clustering-analysis-cont." class="title-slide slide level1 center">
<h1>What is NOT Clustering analysis? (cont.)</h1>
<ul>
<li>Supervised classification
<ul>
<li>It assumes classes to be known</li>
</ul></li>
<li>Segmentation
<ul>
<li>Partitioning students alphabetically by last name</li>
<li>The partitioning rule is given</li>
</ul></li>
<li>Querying a database
<ul>
<li>The selection and grouping criteria are given</li>
</ul></li>
</ul>
<p>How many clusters?</p>
</section>

<section id="cluster-notion-can-be-ambiguous" class="title-slide slide level1 center">
<h1>Cluster notion can be ambiguous</h1>

</section>

<section id="type-of-clustering" class="title-slide slide level1 center">
<h1>Type of clustering</h1>
<ul>
<li>A clustering is a set of clusters. we distinguish between:
<ul>
<li><strong>Partitioning clustering</strong> : a division of objects into non-overlapping subsets (clusters). Each object belongs exactly to a cluster.</li>
<li><strong>Hierarchical clustering</strong> : a set of nested clusters organized as a hierarchical tree</li>
</ul></li>
</ul>
<p>hierarchical clustering</p>
</section>

<section id="further-cluster-classification" class="title-slide slide level1 center">
<h1>Further cluster classification</h1>
<ul>
<li><strong>Exclusive vs non-exclusive</strong>
<ul>
<li>In a non-exclusive clustering, points can belong to multiple clusters.</li>
<li>Useful for representing border points and points belonging to several classes</li>
</ul></li>
<li><strong>Fuzzy vs non-fuzzy</strong>
<ul>
<li>In a fuzzy clustering a point belongs to all clusters with a weight between 0 and 1.</li>
<li>For each point, weights sum up to 1.</li>
</ul></li>
<li><strong>Partial vs complete</strong>
<ul>
<li>In a partial clustering some points may not belong to any of the clusters.</li>
</ul></li>
<li><strong>Heterogeneous vs homogeneous</strong>
<ul>
<li>In a heterogeneous cluster, clusters can have very different sizes, shapes and densities <strong>.</strong></li>
</ul></li>
</ul>
</section>

<section id="cluster-types-well-separated" class="title-slide slide level1 center">
<h1>Cluster types: Well-Separated</h1>
<ul>
<li>Well-Separated Cluster:
<ul>
<li>A cluster is a set of points such that each point in the cluster is closer (more similar to) any other point in the cluster than any other point that does not belong to the cluster.
<ul>
<li>An itemset containing k-elements</li>
</ul></li>
</ul></li>
</ul>
<p>3 well-separated cluster</p>
</section>

<section id="cluster-types-center-based" class="title-slide slide level1 center">
<h1>Cluster types: Center-based</h1>
<ul>
<li>Center-based
<ul>
<li>A cluster is a set of points such that a point in the cluster is closer (or more similar to) to the “center” of the cluster, rather than to the center of each other cluster</li>
<li>Cluster center is called centroid if it is computed as the average of all the cluster’s points, or medoid if it is computed as the “more representative” cluster point</li>
</ul></li>
</ul>
<p>4 center-based cluster</p>
</section>

<section id="cluster-types-contiguity-based" class="title-slide slide level1 center">
<h1>Cluster types: Contiguity-Based</h1>
<ul>
<li>Contiguous cluster (Nearest neighbor or Transitive)
<ul>
<li>A cluster is a set of points such that a point in a cluster is closer (or more similar) to one or more other points in the cluster than to any point not in the cluster</li>
</ul></li>
</ul>
<p>8 contiguous clusters</p>
</section>

<section id="cluster-types-density-based" class="title-slide slide level1 center">
<h1>Cluster types: Density-Based</h1>
<ul>
<li>Density-based
<ul>
<li>A cluster is a dense region of points, which is separated by low-density regions, from other regions of high density.</li>
<li>Used when the clusters are irregular or twisted, and when noise and outliers are present.</li>
</ul></li>
</ul>
<p>6 density-based clusters</p>
</section>

<section id="cluster-types-conceptual-cluster" class="title-slide slide level1 center">
<h1>Cluster types: Conceptual cluster</h1>
<ul>
<li>Clusters with shared properties or in which the shared property derives from the whole set of points (that models a particular concept)
<ul>
<li>Specific techniques are needed to express the underlying concept.</li>
</ul></li>
</ul>
<p>2 overlapped circles</p>
</section>

<section id="k-means-clustering" class="title-slide slide level1 center">
<h1>K-means Clustering</h1>
<p>Partitional clustering approach</p>
<p>Number of clusters, K, must be specified</p>
<p>Each cluster is associated with a centroid (center point)</p>
<p>Each point is assigned to the cluster with the closest centroid</p>
<p>Optimal clusters</p>
<p>Sub-ottimal clusters</p>
</section>

<section id="convergence-and-optimality" class="title-slide slide level1 center">
<h1>Convergence and optimality</h1>
<p>Original points and natural clusters</p>
</section>

<section id="convergence-to-optimality-animated" class="title-slide slide level1 center">
<h1>Convergence to optimality animated</h1>

</section>

<section id="convergence-to-optimality" class="title-slide slide level1 center">
<h1>Convergence to optimality</h1>

</section>

<section id="k-means-clustering-details" class="title-slide slide level1 center">
<h1>K-means Clustering: details</h1>
<ul>
<li>Initial centroids are often chosen randomly.
<ul>
<li>Clusters produced vary from one run to another.</li>
</ul></li>
<li>The centroid is (typically) the mean of the points in the cluster.</li>
<li>‘Closeness’ is measured by Euclidean distance, cosine similarity, correlation, etc.</li>
<li>K-means will converge for common similarity measures mentioned above and most of the convergence happens in the first few iterations.
<ul>
<li>The algorithm can converge to sub-optimal</li>
<li>Often the stopping condition is relaxed to ‘Until relatively few points change clusters’</li>
</ul></li>
<li>Algorithm complexity is O( n * K * I * d )
<ul>
<li>n = number of points, K = number of clusters, I = number of iterations, d = number of attributes</li>
</ul></li>
</ul>
</section>

<section id="evaluating-k-means-clusters" class="title-slide slide level1 center">
<h1>Evaluating k-means clusters</h1>
<ul>
<li>Most common measure is Sum of Squared Error (SSE)
<ul>
<li>For each point, the error is the distance to the nearest cluster</li>
<li>To get SSE, we square these errors and sum them.</li>
<li><em>x</em> is a data point in cluster <em>C</em> <em>i</em> and <em>m</em> <em>i</em> is the representative point for cluster <em>C</em> <em>i</em>
<ul>
<li>The centroid <em>m</em> <em>i</em> that minimizes the SSE when <em>dist</em> _ _ is the Euclidean distance is the average of the cluster points.</li>
</ul></li>
<li>One easy way to reduce SSE is to increase <em>K</em> , the number of clusters
<ul>
<li>A good clustering with smaller K can have a lower SSE than a poor clustering with higher <em>K</em></li>
</ul></li>
</ul></li>
</ul>
</section>

<section id="convergence-and-optimality-1" class="title-slide slide level1 center">
<h1>Convergence and optimality</h1>
<ul>
<li>There is only a finite number of ways to partition <em>n</em> records into <em>k</em> groups. So there is only a finite number of possible configurations in which all the centers are centroids of the points they possess.</li>
<li>If the configuration changes in an iteration, the distortion must get be improved. So every time the configuration changes, it must lead to a state never visited before
<ul>
<li>The reassignment of records to centroids is done on the basis of smaller distances</li>
<li>The calculation of the new centroids minimizes the value of SSE for the cluster</li>
</ul></li>
<li>Therefore the algorithm must stop due to the unavailability of further configurations to visit</li>
<li>It is not necessarily true that the final configuration is the one that in absolute has the minimum value of SSE as shown in the following slide</li>
</ul>
</section>

<section id="convergence-to-sub-optimality-animated" class="title-slide slide level1 center">
<h1>Convergence to sub-optimality animated</h1>

</section>

<section id="convergence-to-sub-optimality-animated-cont." class="title-slide slide level1 center">
<h1>Convergence to sub-optimality animated (cont.)</h1>

</section>

<section id="importance-of-choosing-initial-centroids" class="title-slide slide level1 center">
<h1>Importance of Choosing Initial Centroids</h1>
<ul>
<li>If there are real K clusters the probability of choosing a centroid from each cluster is very limited
<ul>
<li>Assuming the clusters have the same cardinality <em>n</em> :</li>
<li>K = 10, probability is 10!/1010 = 0.00036</li>
<li>Sometimes the centroids will reposition themselves correctly, sometimes not …</li>
</ul></li>
</ul>
</section>

<section id="clusters-example" class="title-slide slide level1 center">
<h1>10 clusters example</h1>
<p>Starting with clusters with 2 centroids and clusters with 0 centroids</p>
</section>

<section id="clusters-example-cont." class="title-slide slide level1 center">
<h1>10 clusters example (cont.)</h1>
<p>Starting with clusters with 2 centroids and clusters with 0 centroids</p>
</section>

<section id="clusters-example-cont.-1" class="title-slide slide level1 center">
<h1>10 clusters example (cont.)</h1>
<p>Starting with a couple of clusters with 3 centroids and couple of clusters with 1 centroids</p>
</section>

<section id="clusters-example-cont.-2" class="title-slide slide level1 center">
<h1>10 clusters example (cont.)</h1>
<p>Starting with a couple of clusters with 3 centroids and couple of clusters with 1 centroids</p>
</section>

<section id="solution-to-the-problems-induced-by-the-choice-of-initial-centroids" class="title-slide slide level1 center">
<h1>Solution to the problems induced by the choice of initial centroids</h1>
<ul>
<li>Run the algorithm several times with different initial centroids
<ul>
<li>It can help, but the probability is not on our side!</li>
</ul></li>
<li>Perform a sampling of the points and use a hierarchical clustering technique to identify k initial centroids</li>
<li>Select more than k initial centroids and then select from these to use
<ul>
<li>The selection criterion is to keep those more “separated”</li>
</ul></li>
<li>Use post-processing techniques to eliminate the identified erroneous cluster</li>
<li>Bisecting K-means
<ul>
<li>Less affected by the problem</li>
</ul></li>
</ul>
</section>

<section id="handling-empty-clusters" class="title-slide slide level1 center">
<h1>Handling empty clusters</h1>
<ul>
<li>The K-means algorithm can determine empty clusters if, during the assignment phase, no element is assigned to a centroid.
<ul>
<li>This case can cause a high SSE as one of the clusters is not “used”</li>
</ul></li>
<li>Different strategies are possible to identify an alternative centroid
<ul>
<li>Choose the item that most contributes to the value of SSE</li>
<li>Choose an item of the cluster with the highest SSE. Normally this causes the cluster to split into two clusters that include the closest elements.</li>
</ul></li>
</ul>
</section>

<section id="handling-empty-clusters-cont." class="title-slide slide level1 center">
<h1>Handling empty clusters (cont.)</h1>

</section>

<section id="handling-outlier" class="title-slide slide level1 center">
<h1>Handling outlier</h1>
<ul>
<li>The goodness of clustering can be negatively influenced by the presence of outliers that tend to “shift” the cluster centroids so that to reduce the increase in the SSE they determine
<ul>
<li>Since SSE is a square of a distance, the far points heavily affect its value</li>
</ul></li>
<li>Outliers, if identified, can be eliminated during preprocessing
<ul>
<li>Outlier concepts depends on the application domain</li>
</ul></li>
</ul>
</section>

<section id="k-means-limits" class="title-slide slide level1 center">
<h1>K-means: limits</h1>
<ul>
<li>The k-means algorithm does not achieve good results when natural clusters have:
<ul>
<li>Different sizes</li>
<li>Different density</li>
<li>Non-globular shape</li>
<li>Data contains outliers</li>
</ul></li>
</ul>
</section>

<section id="k-means-limits-different-dimension" class="title-slide slide level1 center">
<h1>K-means limits: different dimension</h1>
<p>The value of SSE leads to the identification of centroids so as to have clusters of the same size if the clusters are not well-separated</p>
<p><img data-src="img/clustering/14-Clustering_42.png"></p>
<p><img data-src="img/clustering/14-Clustering_43.png"></p>
<p>K-means (3 Cluster)</p>
<p>Unclustered points &amp; natural clusters</p>
</section>

<section id="k-means-limits-different-density" class="title-slide slide level1 center">
<h1>K-means limits: different density</h1>
<p>More dense clusters lead to smaller intra-cluster distances, so less dense areas require more medians to minimize the total value of SSE</p>
<p><img data-src="img/clustering/14-Clustering_44.png"></p>
<p><img data-src="img/clustering/14-Clustering_45.png"></p>
<p>K-means (3 Cluster)</p>
<p>Unclustered points &amp; natural clusters</p>
</section>

<section id="k-means-limits-non-globular-shape" class="title-slide slide level1 center">
<h1>K-means limits: non-globular shape</h1>
<p>SSE is based on an Euclidean distance that does not take into account the shape of objects</p>
<p><img data-src="img/clustering/14-Clustering_46.png"></p>
<p><img data-src="img/clustering/14-Clustering_47.png"></p>
<p>K-means (3 Cluster)</p>
<p>Unclustered points &amp; natural clusters</p>
</section>

<section id="k-means-possible-solutions" class="title-slide slide level1 center">
<h1>K-means: possible solutions</h1>
<p>Use a higher k value, thus identifying portions of clusters.</p>
<p>The definition of “natural” clusters then requires a technique to bring together the identified clusters</p>
<p><img data-src="img/clustering/14-Clustering_48.png"></p>
<p><img data-src="img/clustering/14-Clustering_49.png"></p>
<p>Unclustered points &amp; natural clusters</p>
<p>K-means (3 Cluster)</p>
</section>

<section id="k-means-possible-solutions-cont." class="title-slide slide level1 center">
<h1>K-means: possible solutions (cont.)</h1>
<p>Use a higher k value, thus identifying portions of clusters.</p>
<p>The definition of “natural” clusters then requires a technique to bring together the identified clusters</p>
<p><img data-src="img/clustering/14-Clustering_50.png"></p>
<p><img data-src="img/clustering/14-Clustering_51.png"></p>
<p>Unclustered points &amp; natural clusters</p>
<p>K-means (3 Cluster)</p>
</section>

<section id="k-means-possible-solutions-cont.-1" class="title-slide slide level1 center">
<h1>K-means: possible solutions (cont.)</h1>
<p>Use a higher k value, thus identifying portions of clusters.</p>
<p>The definition of “natural” clusters then requires a technique to bring together the identified clusters</p>
<p><img data-src="img/clustering/14-Clustering_52.png"></p>
<p><img data-src="img/clustering/14-Clustering_53.png"></p>
<p>Unclustered points &amp; natural clusters</p>
<p>K-means (3 Cluster)</p>
</section>

<section id="choosing-k-the-elbow-method" class="title-slide slide level1 center">
<h1>Choosing K: the elbow method</h1>
<ul>
<li>It consists in executing k-means several times with increasing values for k
<ul>
<li>SSE value will decrease</li>
<li>k&lt; #NaturalCluster SSE includes inter-cluster distances</li>
<li>k&gt;= # NaturalCluster SSE includes intra-cluster distances</li>
<li>The “elbow” occurs when SSE starts decreasing slowersince it is generated only by intra-cluster distances</li>
</ul></li>
</ul>

<img data-src="img/clustering/14-Clustering_54.png" class="r-stretch"></section>

<section id="exercise" class="title-slide slide level1 center">
<h1>Exercise</h1>
<ul>
<li>Draw the cluster partitioning and the approximate position of the centroids chosen by the k-means algorithm assuming that:
<ul>
<li>The points are equally distributed</li>
<li>The distance function is SSE</li>
<li>The value of K is shown below the figures</li>
</ul></li>
</ul>

<img data-src="img/clustering/14-Clustering_55.png" class="r-stretch"><p>K=2 K=3 K=3 K=2 K=3</p>
</section>

<section id="hierarchical-clustering" class="title-slide slide level1 center">
<h1>Hierarchical Clustering</h1>
<ul>
<li>Produces a set of nested clusters organized as a hierarchical tree</li>
<li>Can be visualized as a dendrogram
<ul>
<li>A tree like diagram that records the sequences of merges or splits</li>
</ul></li>
<li>Different values on the Y-axis correspond to different clusterings
<ul>
<li>Higher Y-axis values imply less clusters with more items</li>
</ul></li>
</ul>
</section>

<section id="hierarchical-clustering-pros-and-cons" class="title-slide slide level1 center">
<h1>Hierarchical Clustering: pros and cons</h1>
<ul>
<li> The cluster number must not be defined priori
<ul>
<li>The desired number of clusters can be obtained by ‘cutting’ the dendrogram to the appropriate level</li>
</ul></li>
<li> It can identify a taxonomy (hierarchical classification) of concepts
<ul>
<li>The most similar elements will be fused before less similar elements</li>
</ul></li>
<li> Once a decision is made (merge) it can no longer be canceled</li>
<li> In many configurations it is sensitive to noise and outliers</li>
<li> A global optimization function is missing</li>
</ul>
</section>

<section id="hierarchical-clustering-approaches" class="title-slide slide level1 center">
<h1>Hierarchical Clustering approaches</h1>
<ul>
<li>There are two approaches to building a hierarchical clustering
<ul>
<li>Agglomerative:
<ul>
<li>Start with the points as individual clusters</li>
<li>At each step, merge the closest pair of clusters until only one cluster (or k clusters) left</li>
</ul></li>
<li>Divisive:
<ul>
<li>Start with one, all-inclusive cluster</li>
<li>At each step, split a cluster until each cluster contains an individual point (or there are k clusters)</li>
</ul></li>
</ul></li>
<li>Traditional hierarchical algorithms use a similarity or distance matrix
<ul>
<li>A <em>n </em> x <em>n</em> matrix storing the similarities/distances between couples of point/clusters</li>
</ul></li>
</ul>
</section>

<section id="hierarchical-clustering-algorithms" class="title-slide slide level1 center">
<h1>Hierarchical Clustering algorithms</h1>
<ul>
<li>Basic algorithm is straightforward
<ul>
<li>Compute the proximity matrix</li>
<li>Let each data point be a cluster</li>
<li><strong>Repeat</strong></li>
<li>Merge the two closest clusters</li>
<li>Update the proximity matrix</li>
<li><strong>Until</strong> only a single cluster remains</li>
</ul></li>
<li>Key operation is the computation of the proximity of two clusters
<ul>
<li>Different approaches to defining the distance between clusters distinguish the different algorithms</li>
</ul></li>
</ul>
</section>

<section id="hierarchical-clustering-flow" class="title-slide slide level1 center">
<h1>Hierarchical Clustering flow</h1>
<p>Start with clusters of individual points and a proximity matrix</p>
<p>Proximity Matrix</p>
</section>

<section id="hierarchical-clustering-flow-cont." class="title-slide slide level1 center">
<h1>Hierarchical Clustering flow (cont.)</h1>
<p>After some merging steps, we have some clusters</p>
<p>Proximity Matrix</p>
</section>

<section id="merging-step" class="title-slide slide level1 center">
<h1>Merging step</h1>
<ul>
<li>We want to merge the two closest clusters (C2 and C5) and update the proximity matrix.
<ul>
<li>The affected cells are only those related to C2 e C5</li>
</ul></li>
</ul>
<p>Proximity Matrix</p>
</section>

<section id="inter-cluster-distances" class="title-slide slide level1 center">
<h1>Inter-Cluster Distances</h1>
<ul>
<li>MIN or Single link</li>
<li>MAX or Complete link</li>
<li>Group Average</li>
<li>Centroids distance</li>
<li>Other methods driven by an objective function
<ul>
<li>Ward’s Method uses squared error</li>
</ul></li>
</ul>
</section>

<section id="inter-cluster-distances-cont." class="title-slide slide level1 center">
<h1>Inter-Cluster Distances (cont.)</h1>
<ul>
<li><strong>MIN or Single link</strong> : is the minimum distance between two cluster points</li>
<li>MAX or Complete link</li>
<li>Group Average</li>
<li>Centroids distance</li>
<li>Other methods driven by an objective function
<ul>
<li>Ward’s Method uses squared error</li>
</ul></li>
</ul>
</section>

<section id="inter-cluster-distances-cont.-1" class="title-slide slide level1 center">
<h1>Inter-Cluster Distances (cont.)</h1>
<ul>
<li>MIN or Single link: is the minimum distance between two cluster points</li>
<li><strong>MAX or Complete link: </strong> is the maximum distance between all cluster points</li>
<li>Group Average</li>
<li>Centroids distance</li>
<li>Other methods driven by an objective function
<ul>
<li>Ward’s Method uses squared error</li>
</ul></li>
</ul>
</section>

<section id="inter-cluster-distances-cont.-2" class="title-slide slide level1 center">
<h1>Inter-Cluster Distances (cont.)</h1>
<ul>
<li>MIN or Single link: is the minimum distance between two cluster points</li>
<li>MAX or Complete link: is the maximum distance between two cluster points</li>
<li><strong>Group Average</strong> : is the average distance between all the cluster points</li>
<li>Centroids distance</li>
<li>Other methods driven by an objective function
<ul>
<li>Ward’s Method uses squared error</li>
</ul></li>
</ul>
</section>

<section id="inter-cluster-distances-cont.-3" class="title-slide slide level1 center">
<h1>Inter-Cluster Distances (cont.)</h1>
<ul>
<li>MIN or Single link: is the minimum distance between two cluster points</li>
<li>MAX or Complete link: is the maximum distance between two cluster points</li>
<li>Group Average: is the average distance between all the cluster points</li>
<li><strong>Centroids distance</strong></li>
<li>Other methods driven by an objective function
<ul>
<li>Ward’s Method uses squared error</li>
</ul></li>
</ul>
</section>

<section id="hierarchical-clustering-min" class="title-slide slide level1 center">
<h1>Hierarchical clustering: MIN</h1>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">p1</th>
<th style="text-align: center;">p2</th>
<th style="text-align: center;">p3</th>
<th style="text-align: center;">p4</th>
<th style="text-align: center;">p5</th>
<th style="text-align: center;">p6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">p1</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.23</td>
</tr>
<tr class="even">
<td style="text-align: center;">p2</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr class="odd">
<td style="text-align: center;">p3</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr class="even">
<td style="text-align: center;">p4</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr class="odd">
<td style="text-align: center;">p5</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.39</td>
</tr>
<tr class="even">
<td style="text-align: center;">p6</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.00</td>
</tr>
</tbody>
</table>
<p>After step 2…</p>
<p>Dist({3,6}, {2,5})=min(dist({2,3}), dist({2,6}), dist({5,3}), dist({5,6})) = min(0.15,0.25,0.28,0.39)=0.15</p>
</section>

<section id="min-pros-and-cons" class="title-slide slide level1 center">
<h1>MIN: pros and cons</h1>
<p>It allows to manage non-spherical clusters too</p>
<p>It is subject to outliers and noises</p>
<p><img data-src="img/clustering/14-Clustering_59.png"></p>
<p><img data-src="img/clustering/14-Clustering_60.png"></p>
<p><img data-src="img/clustering/14-Clustering_61.png"></p>
<p><img data-src="img/clustering/14-Clustering_62.png"></p>
</section>

<section id="hierarchical-clustering-max" class="title-slide slide level1 center">
<h1>Hierarchical clustering: MAX</h1>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">p1</th>
<th style="text-align: center;">p2</th>
<th style="text-align: center;">p3</th>
<th style="text-align: center;">p4</th>
<th style="text-align: center;">p5</th>
<th style="text-align: center;">p6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">p1</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.23</td>
</tr>
<tr class="even">
<td style="text-align: center;">p2</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr class="odd">
<td style="text-align: center;">p3</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr class="even">
<td style="text-align: center;">p4</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr class="odd">
<td style="text-align: center;">p5</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.39</td>
</tr>
<tr class="even">
<td style="text-align: center;">p6</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.00</td>
</tr>
</tbody>
</table>
<p>After step 2…</p>
<p>Dist({3,6}, {4})=max(dist({3,4}), dist({6,4})) = max(0.15,0.22)=0.22</p>
<p>Dist({3,6}, {2,5})=max(dist({3,2}), dist({3,5}), dist({6,2}) dist({6,5})) = max(0.15,0.25,0.28,0.39)=0.39</p>
<p>Dist({3,6}, {1})=max(dist({3,1}), dist({6,1})) = max(0.22,0.23)=0.23</p>
</section>

<section id="max-pros-and-cons" class="title-slide slide level1 center">
<h1>MAX: pros and cons</h1>
<p>Less affected by noise</p>
<p>It tends to separate large clusters</p>
<p>Favor globular clusters</p>
<p><img data-src="img/clustering/14-Clustering_64.png"></p>
<p><img data-src="img/clustering/14-Clustering_65.png"></p>
<p><img data-src="img/clustering/14-Clustering_66.png"></p>
<p><img data-src="img/clustering/14-Clustering_67.png"></p>
</section>

<section id="hierarchical-clustering-group-average" class="title-slide slide level1 center">
<h1>Hierarchical clustering: Group Average</h1>
<ul>
<li>The similarity between two clusters is the average of the similarity of the cluster pairs of points</li>
<li>It is a compromise between MIN and MAX
<ul>
<li>PROs: less affected by noise than MIN</li>
<li>CONS: Biased towards globular clusters</li>
</ul></li>
</ul>
</section>

<section id="hierarchical-clustering-group-average-cont." class="title-slide slide level1 center">
<h1>Hierarchical clustering: Group Average (cont.)</h1>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">p1</th>
<th style="text-align: center;">p2</th>
<th style="text-align: center;">p3</th>
<th style="text-align: center;">p4</th>
<th style="text-align: center;">p5</th>
<th style="text-align: center;">p6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">p1</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.23</td>
</tr>
<tr class="even">
<td style="text-align: center;">p2</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr class="odd">
<td style="text-align: center;">p3</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr class="even">
<td style="text-align: center;">p4</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr class="odd">
<td style="text-align: center;">p5</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.39</td>
</tr>
<tr class="even">
<td style="text-align: center;">p6</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.00</td>
</tr>
</tbody>
</table>
<p>After step 3…</p>
<p>Dist({3,6,4}, {1}) = (0.22+0.37+0.23)/(31) = 0.28</p>
<p>Dist({2,5}, {1}) = (0.2357+0.3421)/(21) = 0.2889</p>
<p>Dist({3,6,4}, {2,5}) =(0.15+0.28+0.25+0.39+0.20+0.29)/(32) = 0.26</p>
</section>

<section id="hierarchical-clustering-wards-method" class="title-slide slide level1 center">
<h1>Hierarchical clustering: Ward’s Method</h1>
<ul>
<li>Similarity of two clusters is based on the increase in squared error when two clusters are merged
<ul>
<li>Similar to group average if distance between points is distance squared</li>
</ul></li>
<li>PROS: Less susceptible to noise and outliers</li>
<li>CONS: Biased towards globular clusters</li>
<li>It uses the same objective function as the K-means algorithm
<ul>
<li>It can be used to initialize k-means: since it allows identifying the correct value of k and an initial subdivision of the points
<ul>
<li>Please note that not being able to cancel the choices made, the solutions found by the hierarchical methods are often sub-optimal.</li>
</ul></li>
</ul></li>
</ul>
</section>

<section id="hierarchical-clustering-comparison" class="title-slide slide level1 center">
<h1>Hierarchical clustering: Comparison</h1>

</section>

<section id="hierarchical-clustering-computational-complexity" class="title-slide slide level1 center">
<h1>Hierarchical clustering: computational complexity</h1>
<ul>
<li>Space: O(N2) is the space occupied by the proximity matrix when the number of points is N</li>
<li>Time: O(N3)
<ul>
<li>N steps are needed to build the dendrogram. At each step the proximity matrix must be updated and read</li>
<li>Prohibitive for large datasets</li>
</ul></li>
</ul>
</section>

<section id="hierarchical-clustering-exercise" class="title-slide slide level1 center">
<h1>Hierarchical clustering: exercise</h1>
<ul>
<li>Given the following similarity matrix, represent the dendrograms deriving from the hierarchical clustering obtained by
<ul>
<li>Single link</li>
<li>Complete link</li>
</ul></li>
</ul>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: center;">simil</th>
<th style="text-align: center;">p1</th>
<th style="text-align: center;">p2</th>
<th style="text-align: center;">p3</th>
<th style="text-align: center;">p4</th>
<th style="text-align: center;">p5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">p1</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr class="even">
<td style="text-align: center;">p2</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.98</td>
</tr>
<tr class="odd">
<td style="text-align: center;">p3</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr class="even">
<td style="text-align: center;">p4</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr class="odd">
<td style="text-align: center;">p5</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">1.00</td>
</tr>
</tbody>
</table>
</section>

<section id="dbscan" class="title-slide slide level1 center">
<h1>DBSCAN</h1>
<ul>
<li>DBSCAN is a density based approach
<ul>
<li>Density = number of points within a specified radius (Eps)</li>
<li>A point is a <strong>core point </strong> if it has at least a specified number of points (MinPts) within Eps
<ul>
<li>These are points that are at the interior of a cluster</li>
</ul></li>
<li>A <strong>border point </strong> is not a core point, but is in the neighborhood (i.e.&nbsp;within a Eps radius) of a core point</li>
<li>A <strong>noise point </strong> is any point that is not a core point or a border point</li>
</ul></li>
</ul>
</section>

<section id="dbscan-core-border-and-noise-points" class="title-slide slide level1 center">
<h1>DBSCAN: Core, Border and Noise Points</h1>

<img data-src="img/clustering/14-Clustering_69.png" class="r-stretch"></section>

<section id="dbscan-algorithm" class="title-slide slide level1 center">
<h1>DBSCAN algorithm</h1>
<p>// Input:Dataset <strong>D</strong> , MinPts, Eps</p>
<p>// Output set of cluster <strong>C</strong></p>
<p>Label points in <strong>D</strong> as core, border or noise</p>
<p>Drop all noise points</p>
<p>Assign to cluster ci the core points with a distance &lt; Eps from one of the other points assigned to the same cluster</p>
<p>Assign border points to one of the clusters the corresponding core points belong to</p>
</section>

<section id="dbscan-core-border-and-noise-points-1" class="title-slide slide level1 center">
<h1>DBSCAN: Core, Border and Noise Points</h1>
<p>Eps = 10, MinPts = 4</p>
<p><img data-src="img/clustering/14-Clustering_70.png"></p>
<p><img data-src="img/clustering/14-Clustering_71.png"></p>
<p>Point types: core, border and noise</p>
</section>

<section id="dbscan-pros-e-cons" class="title-slide slide level1 center">
<h1>DBSCAN: pros e cons</h1>
<ul>
<li>Pro
<ul>
<li>Resistant to noise</li>
<li>It can generate clusters with different shapes and sizes</li>
</ul></li>
<li>Cons
<ul>
<li>Data with high dimensionality
<ul>
<li>The key point is to properly define the density (i.e.&nbsp;choosing Eps and MinPts)</li>
<li>Does not handle dataset variable density</li>
</ul></li>
</ul></li>
</ul>

<img data-src="img/clustering/14-Clustering_72.png" class="r-stretch"><p>MinPts = 4 Eps = 9.75</p>
<p>MinPts = 4</p>
<p>Eps=9.92</p>
</section>

<section id="dbscan-choosing-eps-and-minpts" class="title-slide slide level1 center">
<h1>DBSCAN: choosing Eps and MinPts</h1>
<ul>
<li>Idea is that for points in a cluster, their kth nearest neighbors are at roughly the same distance</li>
<li>Noise points have the kth nearest neighbor at farther distance</li>
<li>So, plot sorted distance of every point to its kth nearest neighbor
<ul>
<li>Eps value is given by y-axis in p</li>
<li>MinPts value is given by k</li>
<li>The result depends on the value of k, but the trendof the curve remains similar for reasonable values of k</li>
<li>A value of k normally used for two-dimensional datasets is 4</li>
</ul></li>
</ul>

<img data-src="img/clustering/14-Clustering_73.png" class="r-stretch"></section>

<section id="cluster-validity" class="title-slide slide level1 center">
<h1>Cluster validity</h1>
<ul>
<li>For supervised classification techniques there are several measures to evaluate the validity of the results based on the comparison between the known labels of the test set and those calculated by the algorithm
<ul>
<li>Accuracy, precision, recall</li>
</ul></li>
<li>This is not the case of clustering. So why evaluating clustering validity?
<ul>
<li>To avoid finding patterns in noise</li>
<li>To compare clustering algorithms</li>
<li>To compare two sets of clusters</li>
<li>To compare two clusters</li>
</ul></li>
</ul>
</section>

<section id="cluster-validity-cont." class="title-slide slide level1 center">
<h1>Cluster validity (cont.)</h1>
<ul>
<li>For supervised classification techniques there are several measures to evaluate the validity of the results based on the comparison between the known labels of the test set and those calculated by the algorithm
<ul>
<li>Accuracy, precision, recall</li>
</ul></li>
<li>This is not the case of clustering. So why evaluating clustering validity?
<ul>
<li>To avoid finding patterns in noise</li>
<li>To compare clustering algorithms</li>
<li>To compare two sets of clusters</li>
<li>To compare two clusters</li>
</ul></li>
</ul>
</section>

<section id="validity-measures" class="title-slide slide level1 center">
<h1>Validity measures</h1>
<ul>
<li>Numerical measures that are applied to judge various aspects of cluster validity, are classified into the following three types.
<ul>
<li>Internal Index: Used to measure the goodness of a clustering structure without respect to external information.
<ul>
<li>Sum of Squared Error (SSE)</li>
</ul></li>
<li>External Index: Used to measure the extent to which cluster labels match externally supplied class labels.
<ul>
<li>Entropy</li>
</ul></li>
<li>Relative Index: Used to compare two different clusterings or clusters.
<ul>
<li>Often an external or internal index is used for this function, e.g., SSE or entropy</li>
</ul></li>
</ul></li>
</ul>
</section>

<section id="internal-measures" class="title-slide slide level1 center">
<h1>Internal measures</h1>
<ul>
<li>Cluster Cohesion: Measures how closely related are objects in a cluster
<ul>
<li>Example: SSE</li>
</ul></li>
<li>Cluster Separation: Measure how distinct or well-separated a cluster is from other clusters
<ul>
<li>Example: Squared Error</li>
<li>Cohesion is measured by the <strong>W</strong> ithin cluster <strong>S</strong> um of <strong>S</strong> quares (WSS or SSE)</li>
<li>Separation is measured by the <strong>B</strong> etween cluster <strong>S</strong> um of <strong>S</strong> quares (BSS)
<ul>
<li>Where | <em>C</em> <em>i</em> | is the size of cluster <em>i</em></li>
</ul></li>
</ul></li>
</ul>
</section>

<section id="internal-measures-cohesion-and-separation" class="title-slide slide level1 center">
<h1>Internal Measures: Cohesion and Separation</h1>
<ul>
<li>Cohesion and separation can be calculated both for graph-based representations …
<ul>
<li>Cohesion is the sum of the weights of the arcs between the nodes belonging to a cluster</li>
<li>The separation is the sum of the weights of the arcs between the nodes belonging to distinct clusters</li>
</ul></li>
<li>… both for representations based on centroids
<ul>
<li>Cohesion is the sum of the weights of the arcs between the nodes belonging to a cluster and the relative centroid</li>
<li>The separation is the sum of the weights of the arches between the centroids</li>
</ul></li>
</ul>
</section>

<section id="internal-measures-cohesion-and-separation-cont." class="title-slide slide level1 center">
<h1>Internal Measures: Cohesion and Separation (cont.)</h1>
<ul>
<li>Cohesion and separation can be calculated both for graph-based representations …
<ul>
<li>Cohesion is the sum of the weights of the arcs between the nodes belonging to a cluster</li>
<li>The separation is the sum of the weights of the arcs between the nodes belonging to distinct clusters</li>
</ul></li>
<li>… and for representations based on centroids
<ul>
<li>Cohesion is the sum of the weights of the arcs between the nodes belonging to a cluster and the relative centroid</li>
<li>The separation is the sum of the weights of the arches between the centroids</li>
</ul></li>
</ul>
</section>

<section id="measuring-cluster-validity-via-correlation" class="title-slide slide level1 center">
<h1>Measuring Cluster Validity Via Correlation</h1>
<ul>
<li>Two matrices are used
<ul>
<li>Proximity Matrix
<ul>
<li>Distance matrix between elements</li>
</ul></li>
<li>“Incidence” Matrix
<ul>
<li>One row and one column for each data point</li>
<li>An entry is 1 if the associated pair of points belong to the same cluster</li>
<li>An entry is 0 if the associated pair of points belongs to different clusters</li>
</ul></li>
</ul></li>
<li>Compute the correlation between the two matrices</li>
<li>High correlation indicates that points that belong to the same cluster are close to each other.</li>
<li>Not a good measure for some density or contiguity based clusters (such as those obtained with density based algorithms or contiguity measures)
<ul>
<li>In this case the distances between the points are not correlated with their membership of a same cluster</li>
</ul></li>
</ul>
</section>

<section id="measuring-cluster-validity-via-correlation-cont." class="title-slide slide level1 center">
<h1>Measuring Cluster Validity Via Correlation (cont.)</h1>
<ul>
<li>Correlation between the incidence matrix and the proximity matrix on the result of the k-means algorithm on two different data sets.
<ul>
<li>The correlation is negative because at small distances in the proximity matrix they correspond to large values (1) in the incidence matrix</li>
<li>Obviously, if the distance matrix had been used instead of the similarity matrix the correlation would have been positive</li>
</ul></li>
</ul>
</section>

<section id="measuring-cluster-validity-via-correlation-cont.-1" class="title-slide slide level1 center">
<h1>Measuring Cluster Validity Via Correlation (cont.)</h1>
<p>The visualization is obtained by ordering the similarity matrix based on the groupings defined by the clusters.</p>
</section>

<section id="measuring-cluster-validity-via-correlation-cont.-2" class="title-slide slide level1 center">
<h1>Measuring Cluster Validity Via Correlation (cont.)</h1>
<p>If the data is uniformly distributed, the matrix is more “shaded”</p>
</section>

<section id="exercise-1" class="title-slide slide level1 center">
<h1>Exercise</h1>
<p>Associate similarity matrices to data set</p>
<p><img data-src="img/clustering/14-Clustering_88.png"></p>
<p><img data-src="img/clustering/14-Clustering_89.png"></p>
</section>

<section id="cophenetic-distance" class="title-slide slide level1 center">
<h1>Cophenetic distance</h1>
<ul>
<li>The previous measures represent validity indices for partitioning clustering algorithms.</li>
<li>In the case of hierarchical clustering a measure often used is the cophenetic distance, that is, given two elements, is the proximity to which they are placed together by an agglomerative clustering algorithm
<ul>
<li>In the underlying dendrogram, the pairs of points (3,4), (6,4) have a distance of 0.15 because the clusters to which they belong are merged at that value of the proximity matrix</li>
</ul></li>
<li>Calculating the cophenetic distance for all pairs of points, we obtain a matrix that allows us to calculate the CoPhenetic Correlation Coefficient (CPCC)
<ul>
<li>The CPCC is the correlation index between the cophenetic distance matrix and the dissimilarity matrix of the points</li>
<li>A high correlation indicates that the clustering algorithm has respected the dissimilarity between the elements</li>
</ul></li>
</ul>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: center;"><strong>Distance</strong></th>
<th style="text-align: center;">CPCC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Single link</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr class="even">
<td style="text-align: center;">Complete link</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Group average</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr class="even">
<td style="text-align: center;">Ward’s</td>
<td style="text-align: center;">0.64</td>
</tr>
</tbody>
</table>
</section>

<section id="cophenetic-distance-cont." class="title-slide slide level1 center">
<h1>Cophenetic distance (cont.)</h1>
<p>The term <strong>Cophenetic</strong> comes from biology: phenetics (Greek: phainein – to appear), also known as taximetrics, is an attempt to classify organisms based on overall similarity, usually in morphology or other observable traits, regardless of their phylogeny or evolutionary relation.</p>
<ul>
<li>The previous measures represent validity indices for partitioning clustering algorithms.</li>
<li>In the case of hierarchical clustering a measure often used is the cophenetic distance, that is, given two elements, is the proximity to which they are placed together by an agglomerative clustering algorithm
<ul>
<li>In the underlying dendrogram, the pairs of points (3,4), (6,4) have a distance of 0.15 because the clusters to which they belong are merged at that value of the proximity matrix</li>
</ul></li>
<li>Calculating the cophenetic distance for all pairs of points, we obtain a matrix that allows us to calculate the CoPhenetic Correlation Coefficient (CPCC)
<ul>
<li>The CPCC is the correlation index between the cophenetic distance matrix and the dissimilarity matrix of the points</li>
<li>A high correlation indicates that the clustering algorithm has respected the dissimilarity between the elements</li>
</ul></li>
</ul>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: center;"><strong>Distance</strong></th>
<th style="text-align: center;">CPCC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Single link</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr class="even">
<td style="text-align: center;">Complete link</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Group average</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr class="even">
<td style="text-align: center;">Ward’s</td>
<td style="text-align: center;">0.64</td>
</tr>
</tbody>
</table>
</section>

<section id="statistical-framework-for-clustering-validation" class="title-slide slide level1 center">
<h1>Statistical framework for clustering validation</h1>
<ul>
<li>Internal measures often return measurements that must be interpreted
<ul>
<li><em>Is a value of 10 good or bad?</em></li>
</ul></li>
<li>Statistics can provide a suitable methodology for assessing the goodness of a measurement
<ul>
<li>We are looking for non-random patterns, so the more “atypical” the result we get, the more likely it is to represent a non-random pattern in the data</li>
<li>Therefore, the idea is to compare the value of the measure with that obtained from random data.
<ul>
<li>If the value of the measure obtained on the data is unlikely on random data then clustering is valid</li>
</ul></li>
</ul></li>
<li>The issue of interpreting the measure value is less pressing when comparing the result of two clustering</li>
</ul>
</section>

<section id="cluster-in-random-data" class="title-slide slide level1 center">
<h1>Cluster in random data</h1>

</section>

<section id="clustering-tendency" class="title-slide slide level1 center">
<h1>Clustering Tendency</h1>
<ul>
<li>One obvious way to check whether a dataset has clusters is to cluster it
<ul>
<li>Clustering algorithms will still find clusters in the data</li>
<li>There could be types of clusters not identified by the chosen algorithm</li>
</ul></li>
<li>Alternatively, statistical indices, such as the Hopkins statistic, could be used that estimate how evenly distributed the data are
<ul>
<li>Applicable mainly with low dimensionality and in Euclidean spaces</li>
</ul></li>
</ul>
</section>

<section id="clustering-tendency-hopkins-statistic" class="title-slide slide level1 center">
<h1>Clustering Tendency: Hopkins statistic</h1>
<ul>
<li>Given a dataset <strong>Q</strong> of <em>n</em> real points, we generate a dataset <strong>P</strong> of <em>m</em> points ( <em>m</em> &lt;&lt; <em>n</em> ) by randomly distributing them in the data space.</li>
<li>For each point <em>p</em> <em>i</em>  <strong>P</strong> , the distance <em>u</em> <em>i</em> from the point to its nearest-neighbor in the real dataset <strong>Q</strong> is calculated.</li>
<li>Other <em>m</em> points from the real dataset <em>q</em> <em>i</em>  <strong>Q</strong> are sampled and for each of them the distance w <em>i</em> from the point to its nearest-neighbor in the real dataset <strong>Q</strong> is calculated.</li>
<li>The Hopkins statistic is defined as:
<ul>
<li>If the distance to the nearest neighbor is about the same for both generated and sampled points then H  0.5. Then the data set has a random distribution.</li>
<li>If H  1 (very small values of <em>w</em> <em>i</em> ) the data are well clustered</li>
<li>If H  0 (very small <em>u</em> <em>i</em> values) the data are evenly distributed (equally spaced).</li>
</ul></li>
</ul>
</section>

<section id="clustering-tendency-hopkins-statistic-cont." class="title-slide slide level1 center">
<h1>Clustering Tendency: Hopkins statistic (cont.)</h1>
<ul>
<li>Given a dataset <strong>Q</strong> of <em>n</em> real points, we generate a dataset <strong>P</strong> of <em>m</em> points ( <em>m</em> &lt;&lt; <em>n</em> ) by randomly distributing them in the data space.</li>
<li>For each point <em>p</em> <em>i</em>  <strong>P</strong> , the distance <em>u</em> <em>i</em> from the point to its nearest-neighbor in the real dataset <strong>Q</strong> is calculated.</li>
<li>Other <em>m</em> points from the real dataset <em>q</em> <em>i</em>  <strong>Q</strong> are sampled and for each of them the distance w <em>i</em> from the point to its nearest-neighbor in the real dataset <strong>Q</strong> is calculated.</li>
<li>The Hopkins statistic is defined as:
<ul>
<li>If the distance to the nearest neighbor is about the same for both generated and sampled points then H  0.5. Then the data set has a random distribution.</li>
<li>If H  1 (very small values of <em>w</em> <em>i</em> ) the data are well clustered</li>
<li>If H  0 (very small <em>u</em> <em>i</em> values) the data are evenly distributed (equally spaced).</li>
</ul></li>
</ul>
</section>

<section id="clustering-tendency-hopkins-statistic-cont.-1" class="title-slide slide level1 center">
<h1>Clustering Tendency: Hopkins statistic (cont.)</h1>
<ul>
<li>Given a dataset <strong>Q</strong> of <em>n</em> real points, we generate a dataset <strong>P</strong> of <em>m</em> points ( <em>m</em> &lt;&lt; <em>n</em> ) by randomly distributing them in the data space.</li>
<li>For each point <em>p</em> <em>i</em>  <strong>P</strong> , the distance <em>u</em> <em>i</em> from the point to its nearest-neighbor in the real dataset <strong>Q</strong> is calculated.</li>
<li>Other <em>m</em> points from the real dataset <em>q</em> <em>i</em>  <strong>Q</strong> are sampled and for each of them the distance w <em>i</em> from the point to its nearest-neighbor in the real dataset <strong>Q</strong> is calculated.</li>
<li>The Hopkins statistic is defined as:
<ul>
<li>If the distance to the nearest neighbor is about the same for both generated and sampled points then H  0.5. Then the data set has a random distribution.</li>
<li>If H  1 (very small values of <em>w</em> <em>i</em> ) the data are well clustered</li>
<li>If H  0 (very small <em>u</em> <em>i</em> values) the data are evenly distributed (equally spaced).</li>
</ul></li>
</ul>
</section>

<section id="clustering-tendency-hopkins-statistic-cont.-2" class="title-slide slide level1 center">
<h1>Clustering Tendency: Hopkins statistic (cont.)</h1>
<ul>
<li>Given a dataset <strong>Q</strong> of <em>n</em> real points, we generate a dataset <strong>P</strong> of <em>m</em> points ( <em>m</em> &lt;&lt; <em>n</em> ) by randomly distributing them in the data space.</li>
<li>For each point <em>p</em> <em>i</em>  <strong>P</strong> , the distance <em>u</em> <em>i</em> from the point to its nearest-neighbor in the real dataset <strong>Q</strong> is calculated.</li>
<li>Other <em>m</em> points from the real dataset <em>q</em> <em>i</em>  <strong>Q</strong> are sampled and for each of them the distance w <em>i</em> from the point to its nearest-neighbor in the real dataset <strong>Q</strong> is calculated.</li>
<li>The Hopkins statistic is defined as:
<ul>
<li>If the distance to the nearest neighbor is about the same for both generated and sampled points then H  0.5. Then the data set has a random distribution.</li>
<li>If H  1 (very small values of <em>w</em> <em>i</em> ) the data are well clustered</li>
<li>If H  0 (very small <em>u</em> <em>i</em> values) the data are evenly distributed (equally spaced).</li>
</ul></li>
</ul>
</section>

<section id="clustering-tendency-hopkins-statistic-cont.-3" class="title-slide slide level1 center">
<h1>Clustering Tendency: Hopkins statistic (cont.)</h1>
<ul>
<li>Given a dataset <strong>Q</strong> of <em>n</em> real points, we generate a dataset <strong>P</strong> of <em>m</em> points ( <em>m</em> &lt;&lt; <em>n</em> ) by randomly distributing them in the data space.</li>
<li>For each point <em>p</em> <em>i</em>  <strong>P</strong> , the distance <em>u</em> <em>i</em> from the point to its nearest-neighbor in the real dataset <strong>Q</strong> is calculated.</li>
<li>Other <em>m</em> points from the real dataset <em>q</em> <em>i</em>  <strong>Q</strong> are sampled and for each of them the distance w <em>i</em> from the point to its nearest-neighbor in the real dataset <strong>Q</strong> is calculated.</li>
<li>The Hopkins statistic is defined as:
<ul>
<li>If the distance to the nearest neighbor is about the same for both generated and sampled points then H  0.5. Then the data set has a random distribution.</li>
<li>If H  1 (very small values of <em>w</em> <em>i</em> ) the data are well clustered</li>
<li>If H  0 (very small <em>u</em> <em>i</em> values) the data are evenly distributed (equally spaced).</li>
</ul></li>
</ul>
</section>

<section id="clustering-tendency-hopkins-statistic-cont.-4" class="title-slide slide level1 center">
<h1>Clustering Tendency: Hopkins statistic (cont.)</h1>
<ul>
<li>Given a dataset <strong>Q</strong> of <em>n</em> real points, we generate a dataset <strong>P</strong> of <em>m</em> points ( <em>m</em> &lt;&lt; <em>n</em> ) by randomly distributing them in the data space.</li>
<li>For each point <em>p</em> <em>i</em>  <strong>P</strong> , the distance <em>u</em> <em>i</em> from the point to its nearest-neighbor in the real dataset <strong>Q</strong> is calculated.</li>
<li>Other <em>m</em> points from the real dataset <em>q</em> <em>i</em>  <strong>Q</strong> are sampled and for each of them the distance w <em>i</em> from the point to its nearest-neighbor in the real dataset <strong>Q</strong> is calculated.</li>
<li>The Hopkins statistic is defined as:
<ul>
<li>If the distance to the nearest neighbor is about the same for both generated and sampled points then H  0.5. Then the data set has a random distribution.</li>
<li>If H  1 (very small values of <em>w</em> <em>i</em> ) the data are well clustered</li>
<li>If H  0 (very small <em>u</em> <em>i</em> values) the data are evenly distributed (equally spaced).</li>
</ul></li>
</ul>
</section>

<section id="clustering-tendency-sse-comparison" class="title-slide slide level1 center">
<h1>Clustering Tendency: SSE Comparison</h1>
<ul>
<li>The value for SSE of the three clusters below is 0.005</li>
<li>The histogram shows the distribution of SSE for random data
<ul>
<li>500 random points with x and y values between 0.2 and 0.8</li>
<li>The data are clustered into 3 clusters with K-means</li>
</ul></li>
</ul>
</section>

<section id="clustering-tendency-through-correlation" class="title-slide slide level1 center">
<h1>Clustering Tendency: through correlation</h1>
<ul>
<li>The correlation value of the three clusters below is -0.9235</li>
<li>The histogram shows the distribution of the correlation index between the incidence matrix and the proximity matrix for random data
<ul>
<li>500 random points with x and y values between 0.2 - 0.8</li>
<li>The data are clustered into 3 clusters with K-means</li>
</ul></li>
</ul>

<img data-src="img/clustering/14-Clustering_99.png" class="r-stretch"></section>

<section id="external-measures-for-clustering-validation" class="title-slide slide level1 center">
<h1>External measures for clustering validation</h1>
<ul>
<li>External information is usually the class labels of the objects on which clustering is performed
<ul>
<li>They allow you to measure the correspondence between the computed label of the cluster and the class label</li>
</ul></li>
<li>If class labels are available, why perform clustering?
<ul>
<li>Compare the result of different clustering techniques</li>
<li>Evaluate the possibility of automatically obtaining an otherwise manual classification</li>
</ul></li>
<li>Two approaches are possible
<ul>
<li>Classification-oriented: evaluate the extent to which clusters contain objects belonging to the same class
<ul>
<li>Entropy, purity, F-measure</li>
</ul></li>
<li>Similarity-oriented: they measure how often two objects that belong to the same cluster belong to the same class
<ul>
<li>They use similarity measures for binary data: Jaccard</li>
</ul></li>
</ul></li>
</ul>
</section>

<section id="external-measures-for-clustering-validation-entropy-and-purity" class="title-slide slide level1 center">
<h1>External measures for clustering validation: Entropy and Purity</h1>
<ul>
<li>Entropy: for each cluster <em>i</em> let <em>p</em> <em>ij</em> _ _ = <em>m</em> <em>ij</em> _ _ / <em>m</em> <em>i</em> the probability that a member of cluster <em>i</em> belongs to class <em>j</em>
<ul>
<li><em>m</em> <em>i </em> = # of objects in cluster <em>i</em></li>
<li><em>m</em> <em>ij</em> =#of objects of class <em>j</em> in cluster <em>i</em></li>
</ul></li>
<li>Cluster <em>i</em> entropy is <em>e</em> <em>i</em> while whole clustering entropy is <em>e</em>
<ul>
<li><em>L=# class K=# cluster</em></li>
</ul></li>
<li>Purity is computed as:
<ul>
<li><em>p</em> <em>i</em> _ measures how “strong” is the relationship between cluster and class_
<ul>
<li>It is minimal when the points belonging to the cluster are equally distributed among the classes</li>
</ul></li>
</ul></li>
</ul>
</section>

<section id="a-final-comment" class="title-slide slide level1 center">
<h1>A final comment</h1>
<p>“ <em>The validation of clustering structures is the most difficult and frustrating part of cluster analysis. </em></p>
<p>_ Without a strong effort in this direction, cluster analysis will remain a black art accessible only to those true believers who have experience and great courage_ .”</p>
<p>cit. Algorithms for Clustering Data, Jain and Dubes</p>

</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p>Matteo Francia - Machine Learning and Data Mining (Module 2) - A.Y. 2025/26</p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="14-Clustering_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="14-Clustering_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="14-Clustering_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="14-Clustering_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="14-Clustering_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="14-Clustering_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="14-Clustering_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="14-Clustering_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="14-Clustering_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="14-Clustering_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': true,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1280,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>