<!DOCTYPE html>
<html lang="en"><head>
<script src="04-datapreparation_files/libs/quarto-html/tabby.min.js"></script>
<script src="04-datapreparation_files/libs/quarto-html/popper.min.js"></script>
<script src="04-datapreparation_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="04-datapreparation_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="04-datapreparation_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="04-datapreparation_files/libs/quarto-html/quarto-syntax-highlighting-cdaacfc258cb6f151192107f105ac881.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.9.12">

  <meta name="author" content="Matteo Francia   DISI — University of Bologna   m.francia@unibo.it">
  <title>Machine Learning and Data Mining (Module 2)</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="04-datapreparation_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="04-datapreparation_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="04-datapreparation_files/libs/revealjs/dist/theme/quarto-b3aa9dda08c8fde6dffd4aadb76df7d0.css">
  <link href="04-datapreparation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="04-datapreparation_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="04-datapreparation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="04-datapreparation_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning and Data Mining (Module 2)</h1>
  <p class="subtitle">Data Preparation</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Matteo Francia <br> DISI — University of Bologna <br> m.francia@unibo.it 
</div>
</div>
</div>

</section>
<section id="section" class="title-slide slide level1 center">
<h1></h1>

<img src="./img/crispdm_dp.svg" class="center-img r-stretch"></section>

<section id="section-1" class="title-slide slide level1 center">
<h1></h1>
<p><em>Without clean data, the results of a data mining analysis are in question</em>.</p>
</section>

<section id="data-preparation-aka-data-pre-processing" class="title-slide slide level1 center">
<h1>Data Preparation (aka <em>data pre-processing</em>)</h1>
<p>The <strong>data preparation</strong> phase covers all activities to construct the dataset fed into the modeling tools from the initial data.</p>

<img data-src="./img/datapreprocessing/pipeline.svg" class="r-stretch quarto-figure-center"><p class="caption">Data pipeline</p><p>It plays a key role in a data analytics process and <em>avoids “garbage in, garbage out”</em></p>
<ul>
<li><em>A broad range of activities</em>; from correcting errors to selecting the most relevant features
<ul>
<li>Out-of-range values (<code>Income</code>: −100), Impossible combinations (<code>Exam mark</code>: 15 and <code>Exam result</code>: Passed), Missing values…</li>
</ul></li>
<li>Data scientists <em>cannot easily foresee the impact of pipeline prototypes</em>
<ul>
<li>There are <em>no pre-defined rules</em> on the impact of pre-processing transformations</li>
<li>Outline how each quality problem (reported in the earlier “Verify Data Quality” step) has been addressed</li>
</ul></li>
</ul>
</section>

<section id="section-2" class="title-slide slide level1 center">
<h1></h1>
<p>Data pre-processing includes <span class="citation" data-cites="shearer2000crisp">(<a href="#/references" role="doc-biblioref" onclick="">Shearer 2000</a>)</span> data</p>
<ol type="1">
<li><em>selection</em></li>
<li><em>cleansing</em></li>
<li><em>construction</em></li>
<li><em>integration</em></li>
<li><em>formatting</em></li>
</ol>
</section>

<section id="problem-which-data-should-we-use" class="title-slide slide level1 center">
<h1><img src="./img/cs.svg" class="title-icon"> <strong>Problem</strong>: which data should we use?</h1>

</section>

<section id="select-data" class="title-slide slide level1 center">
<h1>Select Data</h1>
<p>Deciding on the data that will be used for the analysis is based on several criteria, including</p>
<ul>
<li>its <em>relevance</em> to the data mining goals</li>
<li><em>quality</em> and <em>technical</em> constraints such as GDPR or limits on data volume or data types</li>
</ul>
<p>Part of the data selection process should involve explaining why certain data was included or excluded.</p>
<ul>
<li>It is also a good idea to decide if one or more attributes are more important than others</li>
</ul>
<blockquote>
<p>Examples</p>
<ul>
<li>If decisions are based on the geographical region, individuals’ addresses can be dropped to reduce the amount of data.</li>
</ul>
<table class="caption-top">
<thead>
<tr class="header">
<th><del><code>Address</code></del></th>
<th><code>Country</code></th>
<th><code>Sales</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Via dell’Università 50</td>
<td>Italy</td>
<td>1000</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<ul>
<li>To learn how sales are characterized by store <code>Type</code>, you do not need to consider the <code>StoreId</code></li>
</ul>
<table class="caption-top">
<thead>
<tr class="header">
<th><del><code>StoreId</code></del></th>
<th><code>Type</code></th>
<th><code>Sales</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S1</td>
<td>grocery</td>
<td>1000</td>
</tr>
<tr class="even">
<td>S2</td>
<td>supermarket</td>
<td>1500</td>
</tr>
<tr class="odd">
<td>S3</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
</blockquote>
</section>

<section id="select-data-1" class="title-slide slide level1 center">
<h1>Select Data</h1>
<p><a href="https://www.kaggle.com/datasets/patrickzel/flight-delay-and-cancellation-dataset-2019-2023">Flight delays dataset</a> contains information about flights in the United States.</p>
<ul>
<li>Task predict flight delays (<code>ARR_DELAY</code>: Arrival delay) based on the following (31) features</li>
<li>Features
<ol type="1">
<li><code>FL_DATE</code>: Date of the flight.</li>
<li><code>AIRLINE</code>: Name of the airline.</li>
<li><code>AIRLINE_DOT</code>: DOT identifier for the airline.</li>
<li><code>AIRLINE_CODE</code>: Code assigned to the airline.</li>
<li><code>DOT_CODE</code>: DOT identifier.</li>
<li><code>FL_NUMBER</code>: Flight number.</li>
<li><code>ORIGIN</code>: Origin airport code.</li>
<li><code>ORIGIN_CITY</code>: City of origin airport.</li>
<li><code>DEST</code>: Destination airport code.</li>
<li><code>DEST_CITY</code>: City of destination airport.</li>
<li><code>DEP_TIME</code>: Actual departure time.</li>
<li><code>DEP_DELAY</code>: Departure delay.</li>
<li><code>ARR_TIME</code>: Actual arrival time.</li>
<li>…</li>
</ol></li>
</ul>
<p>What features would you select/drop to predict flight delays?</p>
</section>

<section id="what-personal-data-is-considered-sensitive" class="title-slide slide level1 center">
<h1><a href="https://commission.europa.eu/law/law-topic/data-protection/reform/rules-business-and-organisations/legal-grounds-processing-data/sensitive-data/what-personal-data-considered-sensitive_en">What personal data is considered sensitive</a>?</h1>
<p>The following personal data is considered <em>sensitive</em> and is subject to specific processing conditions:</p>
<ul>
<li>personal data revealing racial or ethnic origin, political opinions, religious or philosophical beliefs;</li>
<li>trade-union membership;</li>
<li>genetic data, biometric data processed solely to identify a human being;</li>
<li>health-related data;</li>
<li>data concerning a person’s sex life or sexual orientation.</li>
</ul>
<p>See <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32016R0679&amp;from=EN#d1e1489-1-1">article 4</a></p>
<p>Why should we care about sensitive data?</p>
</section>

<section id="the-artificial-intelligence-act" class="title-slide slide level1 center">
<h1>The Artificial Intelligence Act</h1>
<p>The <em>AI Act</em> is a European Union regulation concerning artificial intelligence that classifies applications by their risk of causing harm.</p>
<ul>
<li><strong>Unacceptable-risk</strong> applications are <em>banned</em>.
<ul>
<li>Applications that manipulate human behaviour, use real-time remote biometric identification in public spaces, and social scoring (ranking individuals based on their personal characteristics, socio-economic status, or behaviour)</li>
</ul></li>
<li><strong>High-risk</strong> applications must comply with <em>security</em>, <em>transparency</em> and <em>quality obligations</em>, and undergo conformity assessments.
<ul>
<li>AI applications that are expected to pose significant threats to health, safety, or the fundamental rights of persons.</li>
<li>They must be evaluated both before they are placed on the market and throughout their life cycle.</li>
</ul></li>
<li><strong>Limited-risk</strong> applications <em>only have transparency</em> obligations.
<ul>
<li>AI applications that make it possible to generate or manipulate images, sound, or videos.</li>
<li>Ensure that users are informed that they are interacting with an AI system and are allowed to make informed choices.</li>
</ul></li>
<li><strong>Minimal-risk</strong> applications <em>are not regulated</em>.
<ul>
<li>AI systems used for video games or spam filters.</li>
</ul></li>
</ul>
</section>

<section id="ai-act-and-black-mirror" class="title-slide slide level1 center">
<h1><img src="./img/cs.svg" class="title-icon"> AI Act and Black Mirror</h1>
<p>See <em>Black Mirror</em> episodes and how they relate to the AI Act’s high-risk categories:</p>
<ul>
<li>“The Entire History of You” (Season 1, Episode 3)
<ul>
<li>People have <strong>memory implants</strong> that allow them to replay and analyze past events.</li>
</ul></li>
<li>“Nosedive” (Season 3, Episode 1)
<ul>
<li>People’s <strong>social credit scores</strong> determine their access to housing, jobs, and even flights.</li>
</ul></li>
<li>“Hated in the Nation” (Season 3, Episode 6)
<ul>
<li>A social media campaign with an AI-driven hashtag leads to <strong>automated drone assassinations</strong>.</li>
</ul></li>
<li>“Metalhead” (Season 4, Episode 5)
<ul>
<li>The episode features relentless <strong>autonomous killer robots</strong> that hunt down humans.</li>
</ul></li>
<li>“Rachel, Jack and Ashley Too” (Season 5, Episode 3) – AI &amp; Digital Manipulation
<ul>
<li>A pop star’s <strong>consciousness is cloned into an AI assistant</strong>, and the AI is used to create performances without her consent.</li>
</ul></li>
</ul>
</section>

<section id="problem-missing-values" class="title-slide slide level1 center">
<h1><img src="./img/cs.svg" class="title-icon"> <strong>Problem</strong>: missing values?</h1>
<blockquote>
<p>A retail company tracks daily sales, but some records are missing due to system failures.</p>
<p>A telecom company is predicting customer churn, but some customers have missing contract durations or monthly bill values.</p>
<p>A hospital maintains records of patients’ blood pressure, but 15% of entries are missing.</p>
<p>A bank evaluates loan applications, but some applicants have missing income data.</p>
</blockquote>
</section>

<section id="problem-missing-values-1" class="title-slide slide level1 center">
<h1><img src="./img/cs.svg" class="title-icon"> <strong>Problem</strong>: missing values?</h1>
<blockquote>
<p>A retail company tracks daily sales, but some records are missing due to system failures.</p>
<ul>
<li><em>Use historical sales trends</em> to impute missing values</li>
</ul>
<p>A telecom company is predicting customer churn, but some customers have missing contract durations or monthly bill values.</p>
<ul>
<li><em>Use median imputation for numerical features</em> (e.g., replace missing monthly bill amounts with the median).</li>
</ul>
<p>A hospital maintains records of patients’ blood pressure, but 15% of entries are missing.</p>
<ul>
<li>Use K-Nearest Neighbors (KNN) imputation to <em>estimate missing values based on similar patients</em>.</li>
</ul>
<p>A bank evaluates loan applications, but some applicants have missing income data.</p>
<ul>
<li><em>Use group-based imputation</em> (e.g., average income for self-employed individuals).</li>
</ul>
</blockquote>
</section>

<section id="imputation-of-missing-values" class="title-slide slide level1 center">
<h1>Imputation of missing values</h1>
<p><strong>Imputation</strong> is the process of replacing missing data with substituted values.</p>
<p><em>Listwise deletion</em> (complete case) deletes data with missing values</p>
<ul>
<li>If data are missing at random, listwise deletion does not add any bias, but it decreases the sample size</li>
<li>Otherwise, listwise deletion will introduce bias because the remaining data are not representative of the original sample</li>
</ul>
<div class="columns">
<div class="column" style="width:49%;">
<blockquote>
<p>Before cleaning</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>type</code></th>
<th><code>sales</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S1</td>
<td></td>
<td>1000</td>
</tr>
<tr class="even">
<td>S2</td>
<td>supermarket</td>
<td></td>
</tr>
<tr class="odd">
<td>S3</td>
<td>grocery</td>
<td>100</td>
</tr>
</tbody>
</table>
</blockquote>
</div><div class="column" style="width:49%;">
<blockquote>
<p>After cleaning</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>type</code></th>
<th><code>sales</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S3</td>
<td>grocery</td>
<td>100</td>
</tr>
</tbody>
</table>
</blockquote>
</div></div>
<p><em>Pairwise deletion</em> deletes data when it is missing a variable required for a particular analysis</p>
<ul>
<li>… but includes that data in analyses for which all required variables are present</li>
</ul>
</section>

<section id="imputation-of-missing-values-1" class="title-slide slide level1 center">
<h1>Imputation of missing values</h1>
<p><em>Hot-deck imputation</em>: the information donors come from the same dataset as the recipients</p>
<ul>
<li>One form of hot-deck imputation is called “last observation carried forward”
<ul>
<li>Sort a dataset according to any number of variables, thus creating an ordered dataset</li>
<li>Finds a missing value and uses the value immediately before the data that is missing to impute the missing value</li>
</ul></li>
</ul>
<div class="columns">
<div class="column" style="width:49%;">
<blockquote>
<p>Before cleaning</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>Date</code></th>
<th><code>sales</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S1</td>
<td>2024-10-04</td>
<td>1000</td>
</tr>
<tr class="even">
<td>S1</td>
<td>2024-10-05</td>
<td></td>
</tr>
<tr class="odd">
<td>S2</td>
<td>2024-01-04</td>
<td></td>
</tr>
</tbody>
</table>
</blockquote>
</div><div class="column" style="width:49%;">
<blockquote>
<p>After cleaning (sort by <code>StoreId</code> and <code>Date</code>)</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>Date</code></th>
<th><code>sales</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S1</td>
<td>2024-10-04</td>
<td>1000</td>
</tr>
<tr class="even">
<td><strong>S1</strong></td>
<td><em>2024-10-05</em></td>
<td>1000</td>
</tr>
<tr class="odd">
<td><strong>S2</strong></td>
<td><em>2024-01-04</em></td>
<td>1000</td>
</tr>
</tbody>
</table>
</blockquote>
</div></div>
<p><em>Cold-deck imputation</em> replaces missing values with values from similar data in different datasets</p>
</section>

<section id="imputation-of-missing-values-2" class="title-slide slide level1 center">
<h1>Imputation of missing values</h1>
<p><em>Mean substitution</em> replaces missing values with the mean of that variable for all other cases</p>
<ul>
<li>Mean imputation attenuates any correlations involving the variable(s) that are imputed
<ul>
<li>There is no relationship between the imputed variable and any other measured variables.</li>
<li>Mean imputation can be carried out within classes (i.e., categories, such as gender)</li>
</ul></li>
</ul>
<div class="columns">
<div class="column" style="width:49%;">
<blockquote>
<p>Before cleaning</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>Date</code></th>
<th><code>sales</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S1</td>
<td>2024-10-04</td>
<td>1000</td>
</tr>
<tr class="even">
<td>S1</td>
<td>2024-10-05</td>
<td></td>
</tr>
<tr class="odd">
<td>S1</td>
<td>2024-10-06</td>
<td>2000</td>
</tr>
<tr class="even">
<td>S2</td>
<td>2024-10-04</td>
<td></td>
</tr>
<tr class="odd">
<td>S2</td>
<td>2024-10-05</td>
<td>1000</td>
</tr>
</tbody>
</table>
</blockquote>
</div><div class="column" style="width:49%;">
<blockquote>
<p>After cleaning (average by <code>StoreId</code>)</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>Date</code></th>
<th><code>sales</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S1</td>
<td>2024-10-04</td>
<td>1000</td>
</tr>
<tr class="even">
<td>S1</td>
<td>2024-10-05</td>
<td><em>1500</em></td>
</tr>
<tr class="odd">
<td>S1</td>
<td>2024-10-06</td>
<td>2000</td>
</tr>
<tr class="even">
<td>S2</td>
<td>2024-10-04</td>
<td><em>1000</em></td>
</tr>
<tr class="odd">
<td>S2</td>
<td>2024-10-05</td>
<td>1000</td>
</tr>
</tbody>
</table>
</blockquote>
</div></div>
</section>

<section id="case-study-how-do-we-impute" class="title-slide slide level1 center" data-background-color="#121011">
<h1>Case Study: How Do We Impute?</h1>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: center;">Year</th>
<th style="text-align: center;">Portfolio Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">2008</td>
<td style="text-align: center;">1000.00</td>
</tr>
<tr class="even">
<td style="text-align: center;">2009</td>
<td style="text-align: center;">1050.00</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2010</td>
<td style="text-align: center;">1102.50</td>
</tr>
<tr class="even">
<td style="text-align: center;">2011</td>
<td style="text-align: center;">1157.63</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2012</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2013</td>
<td style="text-align: center;">1276.28</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2014</td>
<td style="text-align: center;">1340.10</td>
</tr>
<tr class="even">
<td style="text-align: center;">2015</td>
<td style="text-align: center;">1407.10</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2016</td>
<td style="text-align: center;">1477.46</td>
</tr>
<tr class="even">
<td style="text-align: center;">2017</td>
<td style="text-align: center;">1551.33</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2018</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2019</td>
<td style="text-align: center;">1710.34</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">1795.86</td>
</tr>
<tr class="even">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">1885.65</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2022</td>
<td style="text-align: center;">1979.93</td>
</tr>
<tr class="even">
<td style="text-align: center;">2023</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2024</td>
<td style="text-align: center;">2182.87</td>
</tr>
</tbody>
</table>
</section>

<section id="case-study-compound-interest" class="title-slide slide level1 center">
<h1>Case Study: Compound Interest</h1>
<blockquote>
<div class="columns">
<div class="column" style="width:70%;">
<p>Our portfolio has an initial value <span class="math inline">\(V_0 = 1000€\)</span> and each has a return of X%</p>
<p>The first year, the portfolio increases its value to <span class="math inline">\(V_1=1000€ + (1000€ \times X\%) = 1050€\)</span></p>
<p>The second year, the portfolio increases its value to <span class="math inline">\(V_2 = V_1 + (V_1 \times X\%) = 1102.50€\)</span></p>
<p>… and so on.</p>
</div><div class="column" style="width:29%;">
<table class="caption-top">
<thead>
<tr class="header">
<th>Year</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1000.00 €</td>
</tr>
<tr class="even">
<td>1</td>
<td>1050.00 €</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1102.50 €</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>18</td>
<td>2406.62 €</td>
</tr>
</tbody>
</table>
</div></div>
</blockquote>
<p>This is not a linear increase but a geometric sequence: <span class="math inline">\(\text{Final value} = \text{Initial value} \times (1 + \frac{r}{n})^\frac{t}{n}\)</span></p>
<ul>
<li><span class="math inline">\(r\)</span> is the nominal annual interest rate</li>
<li><span class="math inline">\(n\)</span> is the compounding frequency (1: annually, 12: monthly, 52: weekly, 365: daily)</li>
<li><span class="math inline">\(t\)</span> is the overall length of time the interest is applied (expressed using the same time units as n, usually years).</li>
</ul>
<blockquote>
<p><span class="math inline">\(\text{Final value} = \text{Initial value} \times (1 + X)^{18}\)</span></p>
<p><span class="math inline">\(X = (\frac{\text{Final value}}{\text{Initial value}})^\frac{1}{18} - 1\)</span></p>
<p><span class="math inline">\(X = 0.05 = 5\%\)</span></p>
</blockquote>
<p>In this case, the return is equal every year, so the average interest is <span class="math inline">\(5\%\)</span>.</p>
</section>

<section id="case-study-compound-interest-1" class="title-slide slide level1 center">
<h1>Case Study: Compound Interest</h1>
<p>Let us assume now that the returns change over the year.</p>
<blockquote>
<table class="caption-top">
<thead>
<tr class="header">
<th>Year</th>
<th>Value</th>
<th>Return</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1000.00 €</td>
<td>-</td>
</tr>
<tr class="even">
<td>1</td>
<td>1050.00 €</td>
<td>5%</td>
</tr>
<tr class="odd">
<td>2</td>
<td>997.50 €</td>
<td>-5%</td>
</tr>
<tr class="even">
<td>3</td>
<td>1047.38 €</td>
<td>5%</td>
</tr>
<tr class="odd">
<td>4</td>
<td>995.01 €</td>
<td>-5%</td>
</tr>
<tr class="even">
<td>5</td>
<td>995.01 €</td>
<td>0%</td>
</tr>
</tbody>
</table>
</blockquote>
<p>What is the average return?</p>
</section>

<section id="case-study-compound-interest-2" class="title-slide slide level1 center">
<h1>Case Study: Compound Interest</h1>
<div class="columns">
<div class="column" style="width:50%;">
<p>If we apply the aritmetic mean it is <span class="math inline">\(\frac{5 -5 + 5 -5 + 0}{5} = 0%\)</span></p>
<p>However, <strong>this is wrong!</strong></p>
</div><div class="column" style="width:50%;">
<blockquote>
<table class="caption-top">
<thead>
<tr class="header">
<th>Year</th>
<th>Value</th>
<th>Return</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1000.00 €</td>
<td>-</td>
</tr>
<tr class="even">
<td>1</td>
<td>1000.00 €</td>
<td>0%</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1000.00 €</td>
<td>0%</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>5</td>
<td>1000.00 €</td>
<td>0%</td>
</tr>
</tbody>
</table>
</blockquote>
</div></div>
<div class="columns">
<div class="column" style="width:50%;">
<p>We already know that <span class="math inline">\(X = \frac{\text{Final value}}{\text{Initial value}}^\frac{1}{5} - 1\)</span></p>
<p>The average return is <span class="math inline">\(X = -0.1\%\)</span></p>
<p><span class="math inline">\(X\%\)</span> is the <em>Compound Annual Growth Rate</em>, the mean annualized growth rate for compounding values over a given time period.</p>
<ul>
<li>CAGR smoothes the effect of the volatility of periodic values that can render arithmetic means less meaningful.</li>
</ul>
</div><div class="column" style="width:50%;">
<blockquote>
<table class="caption-top">
<thead>
<tr class="header">
<th>Year</th>
<th>Value</th>
<th>Return</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1000.00 €</td>
<td>-</td>
</tr>
<tr class="even">
<td>1</td>
<td>999.00 €</td>
<td>-0.1%</td>
</tr>
<tr class="odd">
<td>2</td>
<td>998.00 €</td>
<td>-0.1%</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>5</td>
<td>995.01 €</td>
<td>-0.1%</td>
</tr>
</tbody>
</table>
</blockquote>
</div></div>
</section>

<section id="end-of-the-case-study" class="title-slide slide level1 center" data-background-color="#121011">
<h1>End of the Case Study</h1>
<p><strong>Takeaway</strong>: pay attention to the semantics of the features!</p>
</section>

<section id="problem-how-do-we-handle-anomalies" class="title-slide slide level1 center">
<h1><img src="./img/cs.svg" class="title-icon"> <strong>Problem</strong>: how do we handle anomalies?</h1>
<blockquote>
<p>A customer who typically buys groceries worth $50 suddenly places an order for $5,000 in electronics. Is it a fraud?</p>
<p>A system that usually receives 100-200 requests per second suddenly sees 10,000 requests per second. Is it a DDoS attack?</p>
<p>A bottle-filling machine fills 500ml of liquid, but occasionally, some bottles contain 450ml or 550ml. Is it a defect?</p>
<p>A credit card transaction of $10,000 when the usual spending is around $50-$100. Is it a fraud?</p>
</blockquote>
</section>

<section id="outlier-removal" class="title-slide slide level1 center">
<h1>Outlier removal</h1>
<p><strong>Outlier removal</strong> is the process of eliminating data points that deviate significantly from the rest of the dataset</p>
<ul>
<li>An <em>outlier</em> is a data point that differs significantly from other observations (e.g., a measurement error or heavy-tailed distribution)</li>
</ul>
<div class="columns">
<div class="column" style="width:49%;">
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Standard_deviation_diagram_micro.svg/1920px-Standard_deviation_diagram_micro.svg.png"></p>
</div><div class="column" style="width:25%;">
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Empirical_rule_histogram.svg/1024px-Empirical_rule_histogram.svg.png"></p>
</div></div>
<p>In the case of normally distributed data, the <em>three sigma rule</em> means that:</p>
<ul>
<li>Nearly all values (99.7%) lie within three standard deviations of the mean
<ul>
<li>Roughly 1 in 22 observations will differ by twice the standard deviation or more from the mean,</li>
<li>… and 1 in 370 will deviate by three times the standard deviation.</li>
</ul></li>
<li>If the sample size is only 100, however, just three such outliers are already a reason for concern.</li>
</ul>
</section>

<section id="outlier-removal-1" class="title-slide slide level1 center">
<h1>Outlier removal</h1>
<div class="columns">
<div class="column" style="width:60%;">
<p>Other methods flag outliers based on measures such as the interquartile range.</p>
<p>If <span class="math inline">\(Q_{1}\)</span> and <span class="math inline">\(Q_{3}\)</span> are the lower and upper quartiles, an outlier is any observation outside the range:</p>
<ul>
<li><span class="math inline">\([Q_{1}-k(Q_{3}-Q_{1}),Q_{3}+k(Q_{3}-Q_{1})]\)</span> for some nonnegative <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(k=1.5\)</span> to indicate an “outlier”</li>
<li><span class="math inline">\(k=3\)</span> to indicate data that is “far out”</li>
</ul>
</div><div class="column" style="width:40%;">
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Boxplot_vs_PDF.svg/800px-Boxplot_vs_PDF.svg.png"></p>
</div></div>
</section>

<section id="outlier-removal-2" class="title-slide slide level1 center">
<h1>Outlier removal</h1>
<p><strong>Isolation Forest</strong> <span class="citation" data-cites="liu2008isolation">(<a href="#/references" role="doc-biblioref" onclick="">Liu, Ting, and Zhou 2008</a>)</span> is an algorithm for data anomaly detection using binary trees</p>
<ul>
<li>Because anomalies are few and different from other data, they can be isolated using a few partitions.</li>
<li>Unlike decision tree algorithms, it uses only path length to output an anomaly score.</li>
</ul>
<div class="columns">
<div class="column" style="width:49%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/c/ce/Isolating_a_Non-Anomalous_Point.png"></p>
<figcaption>Inlier</figcaption>
</figure>
</div>
</div><div class="column" style="width:49%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/f/ff/Isolating_an_Anomalous_Point.png"></p>
<figcaption>Outlier</figcaption>
</figure>
</div>
</div></div>
</section>

<section id="case-study-the-black-swan-theory" class="title-slide slide level1 center" data-background-color="#121011">
<h1>Case Study: The Black Swan Theory</h1>
<p>Juvenal (55-128, Roman poet) wrote in his Satire VI of <em>events being “a bird as rare upon the earth as a black swan”</em></p>
<ul>
<li>When the phrase was coined, the black swan was presumed by the Romans not to exist.</li>
<li>All swans are white because all records reported that swans had white feathers.</li>
</ul>
<div class="columns">
<div class="column" style="width:70%;">
<p>In 1697, Dutch explorers became the first Europeans to see black swans in Australia.</p>
<ul>
<li>Observing a <em>single black swan</em> is the undoing of the logic of any system of thought
<ul>
<li>… as well as any reasoning that followed from that underlying logic.</li>
</ul></li>
<li>Conclusions are potentially undone once any of its fundamental postulates is disproved.</li>
</ul>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Black_swan_jan09.jpg/1280px-Black_swan_jan09.jpg"></p>
<figcaption><a href="https://en.wikipedia.org/wiki/Black_swan_theory">Black swan</a></figcaption>
</figure>
</div>
</div></div>
<p>The black swan theory was developed by Nassim Nicholas Taleb <span class="citation" data-cites="taleb2008impact">(<a href="#/references" role="doc-biblioref" onclick="">Taleb 2008</a>)</span> to explain:</p>
<ul>
<li>The disproportionate role of hard-to-predict and rare events that are beyond the realm of normal expectations.</li>
<li>The non-computability of the probability of consequential rare events using scientific methods.</li>
<li>The psychological biases that blind people to uncertainty and to the substantial role of rare events in historical affairs.</li>
</ul>
<p>Such extreme events (outliers) collectively play vastly larger roles than regular occurrences.</p>
</section>

<section id="case-study-the-black-swan-theory-1" class="title-slide slide level1 center" data-background-color="#121011">
<h1>Case Study: The Black Swan Theory</h1>
<p><a href="https://en.wikipedia.org/wiki/Long-Term_Capital_Management">Long-Term Capital Management</a> was a <em>highly leveraged</em> hedge fund.</p>
<ul>
<li>Members of LTCM’s board of directors included Myron Scholes and Robert C. Merton, who shared the Nobel Prize in Economics</li>
<li>LTCM was initially successful, with annualized returns of around 21% in its first year, 43% in its second year, and 41% in its third year.</li>
<li>In 1998, <strong>it lost $4.6 billion</strong> <em>in less than four months due to an unlikely combination</em> of (1997) Asian and (1998) Russian financial crises.</li>
</ul>
<div class="columns">
<div class="column" style="width:60%;">
<blockquote>
<p><span class="citation" data-cites="jorion2000risk">(<a href="#/references" role="doc-biblioref" onclick="">Jorion 2000</a>)</span> […] on 21 August, the portfolio lost $550 million. By 31 August, the portfolio had lost $1,710 million in 1 month.</p>
<ul>
<li>Using the presumed $45 million daily (or $206 million monthly) standard deviation, this translates into an <em>8.3 standard deviation event</em>.</li>
<li>Assuming a normal distribution, <em>such an event would occur once every 800 trillion years, or 40,000 times the age of the universe</em>.</li>
</ul>
<p>Surely this assumption was wrong.</p>
</blockquote>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/e/ec/LTCM.png"></p>
<figcaption>LCTM</figcaption>
</figure>
</div>
</div></div>
</section>

<section id="problem-is-the-dataset-ready-for-machine-learning" class="title-slide slide level1 center">
<h1><img src="./img/cs.svg" class="title-icon"> <strong>Problem</strong>: is the dataset ready for machine learning?</h1>
<blockquote>
<p>An online retailer wants to predict which customers are likely to churn. Instead of using raw purchase data, they need a “Loyalty Score” based on Total purchases in the last 12 months, average order value, and frequency of purchases.</p>
<p>A bank needs to classify customers into risk levels based on their credit score.</p>
<p>Netflix needs to recommend movies based on genre. However, movie genres are categorical (e.g., “Action,” “Comedy”), which must be converted into numbers.</p>
</blockquote>
</section>

<section id="feature-engineering" class="title-slide slide level1 center">
<h1>Feature engineering</h1>
<p><strong>Feature engineering</strong> refers to the manipulation (addition, deletion, combination, mutation) of your data set to improve machine learning model training.</p>
<p><em>Derived attributes</em> should be added if they ease the modeling algorithm</p>
<blockquote>
<p><code>Area</code> = <code>Length</code> x <code>Width</code>.</p>
<p><code>Loyalty_Score</code> = <code>Total_Purchases</code> x 0.4 + <code>Avg_Order_Value</code> x 0.3 + <code>Frequency</code> x 0.3.</p>
<p><code>Ocean_Proximity</code> = distance((<code>Ocean_Latitude</code>, <code>Ocean_Longitude</code>), (<code>District_Latitude</code>, <code>District_Longitude</code>)).</p>
</blockquote>
<p><em>Encoding</em> may be necessary to transform <em>or symbolic fields (“definitely yes”, “yes”, “don’t know”, “no”) to numeric values</em></p>
</section>

<section id="encoding" class="title-slide slide level1 center">
<h1>Encoding</h1>
<p><strong>Encoding</strong> is the process of converting categorical variables into numeric features.</p>
<ul>
<li>Most machine learning algorithms, like linear regression and support vector machines, require input data to be numeric because they use numerical computations to learn the model.</li>
<li>These algorithms are not inherently capable of interpreting categorical data.</li>
<li>Some implementations of decision tree-based algorithms can directly handle categorical data.</li>
</ul>
<p>Categorical features can be <em>nominal</em> or <em>ordinal</em>.</p>
<ul>
<li><em>Nominal features</em> (e.g., colors) do not have a defined ranking or inherent order.</li>
<li><em>Ordinal features</em> (e.g., size) have an inherent order or ranking</li>
</ul>
<p><em>One hot encoding</em> and <em>ordinal encoding</em> are the most common methods to transform categorical variables into numerical features.</p>
</section>

<section id="encoding-ordinal-encoding" class="title-slide slide level1 center">
<h1>Encoding: ordinal encoding</h1>
<p><strong>Ordinal encoding</strong> replaces each category with an integer value.</p>
<ul>
<li>These numbers are, in general, assigned arbitrarily.</li>
<li>Ordinal encoding is a preferred option when the categorical variable has an inherent order.</li>
</ul>
<div class="columns">
<div class="column" style="width:49%;">
<blockquote>
<p>Before encoding</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>ProductId</code></th>
<th><code>Size</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>P1</td>
<td>small</td>
</tr>
<tr class="even">
<td>P2</td>
<td>medium</td>
</tr>
<tr class="odd">
<td>P3</td>
<td>large</td>
</tr>
<tr class="even">
<td>P4</td>
<td>small</td>
</tr>
</tbody>
</table>
</blockquote>
</div><div class="column" style="width:49%;">
<blockquote>
<p>After encoding (small = 0, medium = 1, large = 2)</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>ProductId</code></th>
<th><code>Size</code></th>
<th><code>Size_Enc</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>P1</td>
<td>small</td>
<td>0</td>
</tr>
<tr class="even">
<td>P2</td>
<td>medium</td>
<td>1</td>
</tr>
<tr class="odd">
<td>P3</td>
<td>large</td>
<td>2</td>
</tr>
<tr class="even">
<td>P4</td>
<td>small</td>
<td>0</td>
</tr>
</tbody>
</table>
</blockquote>
</div></div>
</section>

<section id="encoding-likert-scale" class="title-slide slide level1 center">
<h1>Encoding: Likert scale</h1>
<p>The Likert scale is widely used in social work research and is commonly constructed with four to seven points.</p>
<ul>
<li><code>[*, **, ***, ****, *****]</code></li>
<li><code>[1, 2, 3, 4, 5]</code></li>
</ul>
<p>What about averaging?</p>
</section>

<section id="encoding-likert-scale-1" class="title-slide slide level1 center">
<h1>Encoding: Likert scale</h1>
<p>It is usually treated as an interval scale, but strictly speaking, it is an ordinal scale, where arithmetic operations cannot be conducted <span class="citation" data-cites="wu2017can">(<a href="#/references" role="doc-biblioref" onclick="">Wu and Leung 2017</a>)</span></p>
<p>Converting responses to a Likert-type question into an average seems an obvious and intuitive step, but it doesn’t necessarily constitute good methodology. One important point is that respondents are often reluctant to express a strong opinion and may distort the results by gravitating to the neutral midpoint response. It also assumes that the emotional distance between mild agreement or disagreement and strong agreement or disagreement is the same, which isn’t necessarily the case. At its most fundamental level, the problem is that the numbers in a Likert scale are not numbers as such, but a means of ranking responses.</p>
</section>

<section id="encoding-likert-scale-2" class="title-slide slide level1 center">
<h1>Encoding: Likert scale</h1>

<img data-src="./img/datapreprocessing/reviews.png" class="r-stretch quarto-figure-center"><p class="caption">J-shaped distribution</p><p>People tend to write reviews only when they are either extremely satisfied or extremely unsatisfied.</p>
<p>People who feel the product is average might not bother to write a review.</p>
<p><span class="citation" data-cites="hu2009overcoming">(<a href="#/references" role="doc-biblioref" onclick="">Hu, Zhang, and Pavlou 2009</a>)</span></p>
</section>

<section id="encoding-one-hot-encoding" class="title-slide slide level1 center">
<h1>Encoding: one-hot encoding</h1>
<p><strong>One-hot encoding (OHE)</strong> replaces categorical variables by <em>a set of binary variables</em> (each representing a category in the variable)</p>
<ul>
<li>The binary variable takes the value 1 if the observation shows the category, or alternatively, 0.</li>
<li>One hot encoding treats each category independently.</li>
</ul>
<div class="columns">
<div class="column" style="width:49%;">
<blockquote>
<p>Before encoding</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>ProductId</code></th>
<th><code>Color</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>P1</td>
<td>red</td>
</tr>
<tr class="even">
<td>P2</td>
<td>green</td>
</tr>
<tr class="odd">
<td>P3</td>
<td>blue</td>
</tr>
<tr class="even">
<td>P4</td>
<td>red</td>
</tr>
</tbody>
</table>
</blockquote>
</div><div class="column" style="width:49%;">
<p>After encoding</p>
<blockquote>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>ProductId</code></th>
<th><code>Color</code></th>
<th><code>red</code></th>
<th><code>green</code></th>
<th><code>blue</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>P1</td>
<td>red</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>P2</td>
<td>green</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>P3</td>
<td>blue</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>P4</td>
<td>red</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
</blockquote>
</div></div>
<p>OHE increases the dimensionality of the dataset, and it may not be suitable for encoding high cardinality features.</p>
<ul>
<li>To prevent a massive increase in the feature space, we can one-hot encode only the most frequent categories in the variable.</li>
<li>… less frequent values are treated collectively and represented as 0s in all the binary variables.</li>
</ul>
</section>

<section id="case-study-encoding-wrong-data-types" class="title-slide slide level1 center" data-background-color="#121011">
<h1>Case Study: Encoding Wrong Data Types</h1>

<img data-src="https://user-images.githubusercontent.com/18005592/232748093-a25e8ba7-24d4-4e2b-9e58-1553786cac33.png" class="r-stretch quarto-figure-center"><p class="caption">Y2K22 bug</p></section>

<section id="case-study-encoding-wrong-data-types-1" class="title-slide slide level1 center" data-background-color="#121011">
<h1>Case Study: Encoding Wrong Data Types</h1>
<p>Encoding the date 2022-01-01T00:01 into a signed integer <span class="math inline">\(2201010001\)</span></p>
<p>A signed integer is a 32-bit datum that represents an integer in the range:</p>
<ul>
<li>Valid range: <span class="math inline">\([-2^{31}, 2^{31}-1] = [-2147483648, 2147483647]\)</span></li>
<li>However, <span class="math inline">\(2201010001 &gt; 2147483647\)</span></li>
</ul>
<p>See also the <a href="https://en.wikipedia.org/wiki/Year_2000_problem">year 2000 problem</a></p>

<img data-src="https://upload.wikimedia.org/wikipedia/commons/f/fb/Bug_de_l%27an_2000.jpg" class="r-stretch quarto-figure-center"><p class="caption">Y2K</p></section>

<section id="problem-how-do-we-treat-features-with-different-ranges" class="title-slide slide level1 center">
<h1><img src="./img/cs.svg" class="title-icon"> <strong>Problem</strong>: how do we treat features with different ranges?</h1>
<p>If data values vary widely, objective functions will not work properly without normalization in some ML algorithms.</p>
<ul>
<li>For example, many classifiers calculate the distance between two points using the Euclidean distance.
<ul>
<li><span class="math inline">\(d(p,q)={\sqrt {(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\cdots +(p_{n}-q_{n})^{2}}} = \sqrt{\sum_{i=1}^n (p_i - q_i)^2}\)</span></li>
</ul></li>
<li>If one of the features has a broad range of values, the distance will be governed by this particular feature.</li>
</ul>
<blockquote>
<p>Consider a dataset with two features <code>age</code> <span class="math inline">\(\in [0, 120]\)</span> and <code>income</code> <span class="math inline">\(\in [0, 100000]\)</span></p>
<div class="columns">
<div class="column" style="width:60%;">
<p>Given four points</p>
<ul>
<li><span class="math inline">\(p_1=(\)</span><code>age</code> = 50, <code>income</code> = 10000<span class="math inline">\()\)</span></li>
<li><span class="math inline">\(p_2=(\)</span><code>age</code> = 50, <code>income</code> = 20000<span class="math inline">\()\)</span>, <span class="math inline">\(d(p_1,p_2)=10000.00\)</span></li>
<li><span class="math inline">\(p_3=(\)</span><code>age</code> = 60, <code>income</code> = 10000<span class="math inline">\()\)</span>, <span class="math inline">\(d(p_1,p_3)=10.00\)</span></li>
<li><span class="math inline">\(p_4=(\)</span><code>age</code> = 60, <code>income</code> = 20000<span class="math inline">\()\)</span>, <span class="math inline">\(d(p_1,p_4)=10000.00\)</span></li>
</ul>
</div><div class="column" style="width:40%;">
<p><img data-src="./img/datapreprocessing/points.svg"></p>
</div></div>
</blockquote>
</section>

<section id="feature-scaling-or-data-normalization" class="title-slide slide level1 center">
<h1>Feature scaling (or data normalization)</h1>
<p><strong>Feature scaling</strong> normalizes the range of independent variables</p>
<p><em>Min-max normalization</em> rescales the features in <span class="math inline">\([a, b]\)</span> (tipically <span class="math inline">\([0, 1]\)</span>): <span class="math inline">\(x'=a+{\frac{(x-{\text{min}}(x))(b-a)}{{\text{max}}(x)-{\text{min}}(x)}}\)</span></p>
<p><em>Standardization</em> makes the values of each feature in the data have zero-mean and unit-variance: <span class="math inline">\(x'={\frac{x-{\bar {x}}}{\sigma}}\)</span></p>
<p><em>Robust scaling</em> is designed to be robust to outliers: <span class="math inline">\(x'={\frac{x-Q_{2}(x)}{Q_{3}(x)-Q_{1}(x)}}\)</span></p>
<blockquote>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="./img/datapreprocessing/points.svg"></p>
<figcaption>Before min-max normalization</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="./img/datapreprocessing/points_norm.svg"></p>
<figcaption>After min-max normalization</figcaption>
</figure>
</div>
</div></div>
</blockquote>
</section>

<section id="feature-scaling" class="title-slide slide level1 center">
<h1>Feature scaling</h1>
<p><img src="./img/datapreprocessing/clustering.svg" class="center-img" style="!max-height: 500px;"></p>
</section>

<section id="feature-scaling-1" class="title-slide slide level1 center">
<h1>Feature scaling</h1>
<p>Original Iris dataset</p>
<div class="columns">
<div class="column" style="width:24%;">
<p><img data-src="./img/datapreprocessing/normalized-originalpetaldata-distortion-False(6).svg"></p>
</div><div class="column" style="width:24%;">
<p><img data-src="./img/datapreprocessing/normalized-minmaxscaler-distortion-False(3).svg"></p>
</div><div class="column" style="width:24%;">
<p><img data-src="./img/datapreprocessing/normalized-standardscaler-distortion-False(3).svg"></p>
</div><div class="column" style="width:24%;">
<p><img data-src="./img/datapreprocessing/normalized-robustscaler-distortion-False(3).svg"></p>
</div></div>
<p>Transformed Iris dataset: <code>petal_length*=10</code>, addition of 1 outlier [<code>petal_length</code>=100, <code>petal_width</code>=100]</p>
<div class="columns">
<div class="column" style="width:24%;">
<p><img data-src="./img/datapreprocessing/normalized-transformedpetaldata-distortion-True(8).svg"></p>
</div><div class="column" style="width:24%;">
<p><img data-src="./img/datapreprocessing/normalized-minmaxscaler-distortion-True(4).svg"></p>
</div><div class="column" style="width:24%;">
<p><img data-src="./img/datapreprocessing/normalized-standardscaler-distortion-True(4).svg"></p>
</div><div class="column" style="width:24%;">
<p><img data-src="./img/datapreprocessing/normalized-robustscaler-distortion-True(4).svg"></p>
</div></div>
</section>

<section id="what-problems-can-arise-with-skewed-distributions" class="title-slide slide level1 center">
<h1>What problems can arise with skewed distributions?</h1>

<img data-src="https://user-images.githubusercontent.com/18005592/232750742-aacbf6b3-8a7d-49c6-b253-5ab8e7985104.png" class="r-stretch quarto-figure-center"><p class="caption">Skewed vs normal distributions</p></section>

<section id="effects-of-imputation-on-skewed-distributions" class="title-slide slide level1 center">
<h1>Effects of imputation on skewed distributions?</h1>
<p><em>Long Tail</em> refers to the concept where a large number of niche products collectively generate more sales than a few bestsellers.</p>
<ul>
<li>E-commerce sites such as Amazon stock a vast array of products that traditional retailers wouldn’t carry due to space constraints.</li>
<li>The Long Tail phenomenon is directly related to skewed distributions, specifically a type of right-skewed distribution</li>
</ul>

<img data-src="./img/datapreprocessing/imputation_100.svg" class="r-stretch"></section>

<section id="skewed-distributions-what-happens-to-mean-values" class="title-slide slide level1 center">
<h1>Skewed distributions: what happens to mean values?</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="./img/datapreprocessing/height_distribution.svg"></p>
<figcaption>Gaussian distribution</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="./img/datapreprocessing/market_cap_distribution.svg"></p>
<figcaption>Skewed distribution</figcaption>
</figure>
</div>
</div></div>
</section>

<section id="skewed-distributions-what-happens-to-mean-values-1" class="title-slide slide level1 center">
<h1>Skewed distributions: what happens to mean values?</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="./img/datapreprocessing/height_distribution.svg"></p>
<figcaption>Gaussian distribution</figcaption>
</figure>
</div>
<pre><code>Mean   = 173 cm
Median = 173 cm</code></pre>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="./img/datapreprocessing/market_cap_distribution.svg"></p>
<figcaption>Skewed distribution</figcaption>
</figure>
</div>
<pre><code>Mean:   103158262914
Median:  36666524821</code></pre>
</div></div>
</section>

<section id="skewed-distributions" class="title-slide slide level1 center">
<h1>Skewed distributions</h1>
<p>Skewed distributions can be transformed using mathematical functions such as the logarithm.</p>
<p><img src="./img/datapreprocessing/income_histograms.svg" class="center-img" style="!max-height: 500px;"></p>
</section>

<section id="skewed-distributions-1" class="title-slide slide level1 center">
<h1>Skewed distributions</h1>
<p><img src="./img/datapreprocessing/clustering_figures.svg" class="center-img" style="!max-height: 500px;"></p>
</section>

<section id="problem-if-the-dataset-is-too-detailednoisy-what-can-we-do" class="title-slide slide level1 center">
<h1><img src="./img/cs.svg" class="title-icon"> <strong>Problem</strong>: if the dataset is too detailed/noisy, what can we do?</h1>
<blockquote>
<p>Some machine learning models can only work with numerical values.</p>
<p>How do we transform the categorical values of the relevant features into numerical ones?</p>
</blockquote>
<p><em>Aggregation</em> and <em>Binning</em> may be necessary to <em>transform ranges to symbolic fields</em></p>
<div class="columns">
<div class="column" style="width:49%;">
<blockquote>
<p>Before binning</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>Date</code></th>
<th><code>sales</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S1</td>
<td>2024-10-04</td>
<td>1000</td>
</tr>
<tr class="even">
<td>S1</td>
<td>2024-10-05</td>
<td>1500</td>
</tr>
<tr class="odd">
<td>S1</td>
<td>2024-10-06</td>
<td>2000</td>
</tr>
</tbody>
</table>
</blockquote>
</div><div class="column" style="width:49%;">
<blockquote>
<p>After binning (every 1000€)</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>Date</code></th>
<th><code>sales</code></th>
<th><code>sales_bin</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S1</td>
<td>2024-10-04</td>
<td>1000</td>
<td>[1000-2000)</td>
</tr>
<tr class="even">
<td>S1</td>
<td>2024-10-05</td>
<td>1500</td>
<td>[1000-2000)</td>
</tr>
<tr class="odd">
<td>S1</td>
<td>2024-10-06</td>
<td>2000</td>
<td>[2000-3000)</td>
</tr>
</tbody>
</table>
</blockquote>
</div></div>
</section>

<section id="problem-if-the-dataset-is-too-detailednoisy-what-can-we-do-1" class="title-slide slide level1 center">
<h1><img src="./img/cs.svg" class="title-icon"> <strong>Problem</strong>: if the dataset is too detailed/noisy, what can we do?</h1>
<blockquote>
<p>A meteorologist analyzes hourly temperature readings, but the data has fluctuations due to temporary weather conditions.</p>
<table class="caption-top">
<thead>
<tr class="header">
<th>Hour</th>
<th>Temperature (°C)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>24.1</td>
</tr>
<tr class="even">
<td>2</td>
<td>24.3</td>
</tr>
<tr class="odd">
<td>3</td>
<td>23.8</td>
</tr>
<tr class="even">
<td>4</td>
<td>24.5</td>
</tr>
<tr class="odd">
<td>5</td>
<td>22.9 (Sudden drop due to rain)</td>
</tr>
<tr class="even">
<td>6</td>
<td>24.2</td>
</tr>
<tr class="odd">
<td>7</td>
<td>23.9</td>
</tr>
</tbody>
</table>
<p>How can we smooth small variations?</p>
</blockquote>
</section>

<section id="problem-if-the-dataset-is-too-detailednoisy-what-can-we-do-2" class="title-slide slide level1 center">
<h1><img src="./img/cs.svg" class="title-icon"> <strong>Problem</strong>: if the dataset is too detailed/noisy, what can we do?</h1>
<blockquote>
<p>For instance, using <em>equal-width binning</em> (grouping every 3 hours and averaging):</p>
<table class="caption-top">
<thead>
<tr class="header">
<th>Time Period</th>
<th>Smoothed Temperature (°C)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1-3 AM</td>
<td>24.0 (Avg of 24.1, 24.3, 23.8)</td>
</tr>
<tr class="even">
<td>4-6 AM</td>
<td>23.9 (Avg of 24.5, 22.9, 24.2)</td>
</tr>
<tr class="odd">
<td>7 AM</td>
<td>23.9</td>
</tr>
</tbody>
</table>
<p>Noise from sudden drops (e.g., 22.9°C at 5 AM) is smoothed, making temperature trends more reliable.</p>
</blockquote>
</section>

<section id="aggregation" class="title-slide slide level1 center">
<h1>Aggregation</h1>
<p><strong>Aggregation</strong> computes new values by summarizing information from multiple records and/or tables.</p>
<blockquote>
<p>For example, converting a table of product purchases, where there is one record for each purchase, into a new table where there is one record for each store.</p>
</blockquote>
<div class="columns">
<div class="column" style="width:49%;">
<blockquote>
<p>Before aggregation (detailed data)</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>ProductId</code></th>
<th><code>sales</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S1</td>
<td>P1</td>
<td>750</td>
</tr>
<tr class="even">
<td>S2</td>
<td>P1</td>
<td>250</td>
</tr>
<tr class="odd">
<td>S3</td>
<td>P2</td>
<td>…</td>
</tr>
</tbody>
</table>
</blockquote>
</div><div class="column" style="width:49%;">
<blockquote>
<p>After aggregation (sum sales by store)</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>sales</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S1</td>
<td>1000</td>
</tr>
<tr class="even">
<td>S2</td>
<td>1500</td>
</tr>
<tr class="odd">
<td>S3</td>
<td>…</td>
</tr>
</tbody>
</table>
</blockquote>
</div></div>
<p>Pay attention to the <em>aggregation operator</em>!</p>
<ul>
<li>Correct: sum of sums
<ul>
<li><span class="math inline">\((1 + 2) + (3 + 4 + 5) = 1 + 2 + 3 + 4 + 5 = 15\)</span></li>
</ul></li>
<li>Wrong: average of averages
<ul>
<li><span class="math inline">\(avg(avg(1, 2), avg(3, 4, 5)) = avg(1.5, 4) = 2.75\)</span></li>
<li><span class="math inline">\(avg(1, 2, 3, 4, 5) = 3\)</span></li>
</ul></li>
</ul>
</section>

<section id="binning" class="title-slide slide level1 center">
<h1>Binning</h1>
<p><strong>Data binning</strong> is a data pre-processing technique that reduces the effects of minor observation errors</p>
<ul>
<li>The original values that fall into a given interval (bin) are replaced by a central value representative of that interval</li>
<li>Histograms are an example of data binning used in order to observe underlying frequency distributions</li>
</ul>
<p><em>Equal-width</em>: divide the range of values into equal-sized intervals or bins</p>
<ul>
<li>For example, if the values range from 0 to 100, and we want 10 bins, each bin will have a width of 10</li>
<li>It can create empty or sparse bins, especially if the data is skewed or has outliers</li>
</ul>
<p><em>Equal-frequency</em>: divide the values into bins that have the same number of observations or frequency</p>
<ul>
<li>For example, if we have 100 observations and we want 10 bins, each bin will have 10 observations</li>
<li>It creates balanced bins that can handle skewed data and outliers better</li>
<li>The disadvantage is that it can distort the distribution of the data and create irregular bin widths</li>
</ul>
</section>

<section id="section-3" class="title-slide slide level1 center">
<h1></h1>
<p><img data-src="./img/datapreprocessing/imputation_100.svg"></p>
<p><img data-src="./img/datapreprocessing/imputation_20.svg"></p>
</section>

<section id="section-4" class="title-slide slide level1 center">
<h1></h1>
<p><img data-src="./img/datapreprocessing/imputation_20.svg"></p>
<p><img data-src="./img/datapreprocessing/imputation_equi_frequency.svg"></p>
</section>

<section id="problem-what-if-we-have-too-many-features" class="title-slide slide level1 center">
<h1><img src="./img/cs.svg" class="title-icon"> <strong>Problem</strong>: what if we have too many features?</h1>
<blockquote>
<p>A streaming platform wants to recommend movies based on user preferences.</p>
<p>Each movie is represented by a vector of features:</p>
<ol type="1">
<li><code>Genre</code></li>
<li><code>Director</code></li>
<li><code>Lead Actor</code></li>
<li><code>IMDB Rating</code></li>
<li><code>Budget</code></li>
<li><code>User Reviews</code></li>
<li><code>Box Office Revenue</code></li>
<li><code>Soundtrack Style</code></li>
<li>… and many more (let’s assume 100+ features per movie).</li>
</ol>
<p>If movies had only 2 features (e.g., <code>Genre</code> and <code>IMDB Rating</code>), we could easily visualize clusters of similar movies.</p>
<p>With 100+ features, the data points are spread out across a vast space.</p>
<ul>
<li>All points seem “far apart” from each other, making similarity calculations less reliable.</li>
<li>Nearest neighbors are not actually close (because all distances become similar).</li>
</ul>
</blockquote>
</section>

<section id="dimensionality-reduction" class="title-slide slide level1 center">
<h1>Dimensionality reduction</h1>
<p><strong>Dimensionality reduction</strong> is the transformation of data from a high-dimensional space into a low-dimensional space</p>
<ul>
<li>Working in high-dimensional spaces can be undesirable for many reasons</li>
<li>Raw data are often sparse as a consequence of the curse of dimensionality</li>
<li>Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or to facilitate other analyses</li>
</ul>
<p>The main approaches can also be divided into <em>feature selection</em> and <em>feature extraction</em>.</p>
</section>

<section id="feature-selection" class="title-slide slide level1 center">
<h1>Feature selection</h1>
<p><strong>Feature selection</strong> is the process of selecting a subset of relevant features (variables, predictors) for use in model construction</p>
<div class="columns">
<div class="column" style="width:60%;">
<p><em>Dummy algorithm</em>: test each subset of features to find the one that minimizes the error</p>
<ul>
<li><p>This is an exhaustive search of the space, and is computationally intractable for all but the smallest of feature sets</p></li>
<li><p>If <span class="math inline">\(S\)</span> is a finite set of features with cardinality <span class="math inline">\(|S|\)</span>, then the number of all the subsets of <span class="math inline">\(S\)</span> is <span class="math inline">\(|P(S)| = 2^{|S|} - 1\)</span> (do not consider <span class="math inline">\(\varnothing\)</span>)</p>
<ul>
<li>With 3 features: <span class="math inline">\(2^3=8\)</span> subsets</li>
<li>With 4 features: <span class="math inline">\(2^4=16\)</span> subsets</li>
<li>With 10 features: <span class="math inline">\(2^{10}=1024\)</span> subsets</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Hasse_diagram_of_powerset_of_3.svg/1280px-Hasse_diagram_of_powerset_of_3.svg.png"></p>
</div></div>
<p>Feature selection approaches are characterized by</p>
<ul>
<li><em>Search technique</em> for proposing new feature subsets</li>
<li><em>Evaluation measure</em> for scoring the different feature subsets</li>
</ul>
</section>

<section id="feature-selection-1" class="title-slide slide level1 center">
<h1>Feature selection</h1>
<div class="columns">
<div class="column" style="width:65%;">
<p><strong>Feature selection</strong> approaches try to find a subset of the input variables</p>
<p><em>Filter</em> strategy: select variables regardless of the model</p>
<ul>
<li>Based only on general features like the correlation with the variable to predict</li>
</ul>
<p><em>Wrapper</em> strategy</p>
<ul>
<li>Methods include forward selection, backward elimination, and exhaustive search</li>
</ul>
<p><em>Embedded</em> strategy</p>
<ul>
<li>Add/remove features while building the model based on prediction errors</li>
<li>A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously</li>
</ul>
</div><div class="column" style="width:34%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/2/2c/Filter_Methode.png"></p>
<figcaption>Filter</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/0/04/Feature_selection_Wrapper_Method.png"></p>
<figcaption>Wrapper</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/b/bf/Feature_selection_Embedded_Method.png"></p>
<figcaption>Embedded</figcaption>
</figure>
</div>
</div></div>
</section>

<section id="feature-selection-filter-strategy" class="title-slide slide level1 center">
<h1>Feature selection: Filter strategy</h1>
<p><em>Variance threshold</em></p>
<ul>
<li>Mean: <span class="math inline">\(\mu = \frac{1}{n} \sum _{i=1}^{n}x_{i}\)</span>, Variance: <span class="math inline">\(Var(X)={\frac{1}{n}}\sum _{i=1}^{n}(x_{i}-\mu)^{2}\)</span></li>
<li>Features with low variance do not contribute much information to a model.</li>
<li>Use a variance threshold to remove any features that have little to no variation in their values.</li>
<li>Since variance can only be calculated on numeric values, this method only works on quantitative features.</li>
</ul>
<div class="columns">
<div class="column" style="width:33%;">
<blockquote>
<p>Before selection</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>sales</code></th>
<th><code>PostalCode</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1000</td>
<td>47522</td>
</tr>
<tr class="even">
<td>2</td>
<td>1500</td>
<td>47522</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1000</td>
<td>47522</td>
</tr>
</tbody>
</table>
</blockquote>
</div><div class="column" style="width:33%;">
<blockquote>
<p>Compute variance</p>
<p><span class="math inline">\(VAR(\)</span><code>StoreId</code><span class="math inline">\()=0.67\)</span> <strong>(?)</strong></p>
<p><span class="math inline">\(VAR(\)</span><code>sales</code><span class="math inline">\()=55555.56\)</span></p>
<p><span class="math inline">\(VAR(\)</span><code>PostalCode</code><span class="math inline">\()=0\)</span></p>
</blockquote>
</div><div class="column" style="width:33%;">
<blockquote>
<p>After selection (<span class="math inline">\(VAR(X) &gt; 0.6\)</span>)</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>sales</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1000</td>
</tr>
<tr class="even">
<td>2</td>
<td>1500</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1000</td>
</tr>
</tbody>
</table>
</blockquote>
</div></div>
</section>

<section id="feature-selection-filter-strategy-1" class="title-slide slide level1 center">
<h1>Feature selection: Filter strategy</h1>
<div class="columns">
<div class="column" style="width:60%;">
<p><em>Pearson’s correlation</em>: measures the <em>linear</em> relationship between 2 numeric variables</p>
<ul>
<li>A coefficient close to 1 represents a positive correlation, -1 a negative correlation, and 0 no correlation</li>
<li><em>Correlation between features</em>:
<ul>
<li>When two features are highly correlated with one another, then keeping just one to be used in the model will be enough</li>
<li>The second variable would only be redundant and serve to contribute unnecessary noise.</li>
</ul></li>
<li><em>Correlation between feature and target</em>:
<ul>
<li>If a feature is not very correlated with the target variable, such as having a coefficient of between -0.3 and 0.3, then it may not be very predictive and can potentially be filtered out.</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<p><img data-src="https://www.w3schools.com/datascience/img_stat_heatmap.png"></p>
</div></div>
</section>

<section id="feature-selection-wrapper-strategy" class="title-slide slide level1 center">
<h1>Feature selection: Wrapper strategy</h1>
<p><em>Each new feature subset is used to train a model</em>, which is tested on a hold-out set</p>
<ul>
<li>Counting the number of mistakes made on that hold-out set (the error rate of the model) gives the score for that subset</li>
<li>As wrapper methods train a new model for each subset, they are very computationally intensive but provide good results</li>
</ul>
<p><em>Stepwise regression</em> adds the best feature (or deletes the worst feature) at each round</p>
<p><em>Backward elimination</em></p>
<ul>
<li><p>Start with the full model (including all features) and then incrementally remove the most insignificant feature.</p></li>
<li><p>This process repeats again and again until we have the final set of significant features.</p>
<ol type="1">
<li>Choose a significance level (e.g., SL = 0.05 with a 95% confidence).</li>
<li>Fit a full model including all the features.</li>
<li>Consider the feature with the highest p-value.
<ul>
<li>If the p-value &lt; SL, terminate the process.</li>
</ul></li>
<li>Remove the feature that is under consideration.</li>
<li>Fit a model without this feature. Repeat the entire process from Step 3.</li>
</ol></li>
</ul>
</section>

<section id="feature-selection-embedded-strategy" class="title-slide slide level1 center">
<h1>Feature selection: Embedded strategy</h1>
<div class="columns">
<div class="column" style="width:65%;">
<p>Linear regression model: <span class="math inline">\(\hat{y}_i = \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p\)</span></p>
<ul>
<li>Goal is minimizing the sum of squared errors between predicted/actual values</li>
<li><span class="math inline">\(min(\sum_{i=1}^n​(y_i​ - \hat{y}_i​)^2)\)</span>
<ul>
<li><span class="math inline">\(y_i\)</span>​ (red) is the actual value, <span class="math inline">\(\hat{y}_i\)</span>​ is the predicted value (blue)</li>
</ul></li>
</ul>
<p><em>Least Absolute Shrinkage and Selection Operator (LASSO)</em></p>
<ul>
<li>Lasso adds a penalty proportional to the absolute values of the coefficients.</li>
<li><span class="math inline">\(min(\sum_{i=1}^n​(y_i​ - \hat{y}_i​)^2+ \lambda \sum_{j=1}^p​ ∣\beta_j​∣)\)</span>
<ul>
<li><span class="math inline">\(\beta_j\)</span>​ are the coefficients of the model,</li>
<li><span class="math inline">\(\lambda\)</span> is the regularization parameter controlling the penalty’s strength.</li>
</ul></li>
</ul>
<p>Lasso performs automatic feature selection.</p>
<ul>
<li>By shrinking some coefficients to 0, Lasso removes irrelevant features</li>
</ul>
<p>The optimal <span class="math inline">\(\lambda\)</span> can be determined with cross-validation techniques.</p>
<p>See also <span class="citation" data-cites="katrutsa2017comprehensive">(<a href="#/references" role="doc-biblioref" onclick="">Katrutsa and Strijov 2017</a>)</span> <span class="citation" data-cites="chan2022mitigating">(<a href="#/references" role="doc-biblioref" onclick="">Chan et al. 2022</a>)</span></p>
</div><div class="column" style="width:34%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Linear_least_squares_example2.svg/1024px-Linear_least_squares_example2.svg.png"></p>
<figcaption>Linear regression</figcaption>
</figure>
</div>
<p><img data-src="./img/datapreprocessing/modelcomplexity.png"></p>
</div></div>
</section>

<section id="feature-extraction-or-feature-projection" class="title-slide slide level1 center">
<h1>Feature extraction (or feature projection)</h1>
<p><strong>Feature projection</strong> transforms the data from the high-dimensional space to a space of fewer dimensions</p>
<ul>
<li>The data transformation may be linear, as in principal component analysis (PCA)</li>
<li>… but many nonlinear dimensionality reduction techniques also exist</li>
</ul>
<p><em>Principal component analysis (PCA)</em> is a linear dimensionality reduction technique.</p>
<ul>
<li>PCA aims to preserve as much of the data’s variance as possible in fewer dimensions</li>
<li>Variance measures how much the data points differ from the mean of the dataset</li>
<li>The data is linearly transformed onto a new coordinate system such that the directions (<em>principal components</em>) capturing the largest variation in the data can be easily identified</li>
<li>The first principal component captures the highest variance, the second component captures the second highest, and so on.</li>
</ul>
<p>Computing PCA</p>
<ul>
<li>PCA is sensitive to the scale of the data. The first step is usually to standardize the features (mean = 0, standard deviation = 1) to ensure that all features contribute equally to the analysis.</li>
<li>Then, compute the covariance Matrix
<ul>
<li>Eigenvectors represent the directions of the principal components.</li>
<li>Eigenvalues represent the magnitude of variance in the direction of the corresponding eigenvector.</li>
</ul></li>
<li>The eigenvector with the largest eigenvalue is the first principal component, and so on.</li>
</ul>
</section>

<section id="pca-on-the-iris-dataset" class="title-slide slide level1 center">
<h1>PCA on the Iris dataset</h1>
<div class="columns">
<div class="column" style="width:60%;">
<p>Iris contains 4 features; we cannot plot it directly.</p>
<ol type="1">
<li><code>petal_length</code></li>
<li><code>petal_width</code></li>
<li><code>sepal_length</code></li>
<li><code>sepal_width</code></li>
</ol>
<p><img data-src="./img/datapreprocessing/pca.svg"></p>
</div><div class="column" style="width:40%;">
<table class="caption-top">
<thead>
<tr class="header">
<th>Principal Component</th>
<th>Explained Variance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PC 1</td>
<td>92.46%</td>
</tr>
<tr class="even">
<td>PC 2</td>
<td>5.31%</td>
</tr>
<tr class="odd">
<td>PC 3</td>
<td>1.71%</td>
</tr>
</tbody>
</table>
<p>Feature Relevance for 3 Components:</p>
<table class="caption-top">
<thead>
<tr class="header">
<th>Feature</th>
<th>PC 1</th>
<th>PC 2</th>
<th>PC 3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sepal Length (cm)</td>
<td>0.361</td>
<td>0.657</td>
<td>-0.582</td>
</tr>
<tr class="even">
<td>Sepal Width (cm)</td>
<td>-0.085</td>
<td>0.730</td>
<td>0.598</td>
</tr>
<tr class="odd">
<td>Petal Length (cm)</td>
<td>0.857</td>
<td>-0.173</td>
<td>0.076</td>
</tr>
<tr class="even">
<td>Petal Width (cm)</td>
<td>0.358</td>
<td>-0.075</td>
<td>0.546</td>
</tr>
</tbody>
</table>
</div></div>
</section>

<section id="problem-how-do-we-integrate-different-data-sources" class="title-slide slide level1 center">
<h1><img src="./img/cs.svg" class="title-icon"> <strong>Problem</strong>: how do we integrate different data sources?</h1>
<blockquote>
<p>A hospital wants to analyze patient health records by integrating data from multiple sources, including electronic health records (EHRs), wearable devices, and insurance claims.</p>
</blockquote>
</section>

<section id="integrate-data" class="title-slide slide level1 center">
<h1>Integrate Data</h1>
<p><strong>Integration</strong> involves <em>combining information from multiple tables</em> or records to create new records or values.</p>
<ul>
<li>With table-based data, an analyst can join two or more tables that have different information about the same objects.</li>
</ul>
<blockquote>
<p>For instance, a retail chain has one table with information about each store’s general characteristics (e.g., floor space, type of mall), another table with summarized sales data (e.g., profit, percent change in sales from the previous year), and another table with information about the demographics of the surrounding area.</p>
<p>These tables can be merged together into a new table with one record for each store.</p>
</blockquote>
<div class="columns">
<div class="column" style="width:30%;">
<blockquote>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>Type</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S1</td>
<td>grocery</td>
</tr>
<tr class="even">
<td>S2</td>
<td>supermarket</td>
</tr>
<tr class="odd">
<td>S3</td>
<td>…</td>
</tr>
</tbody>
</table>
</blockquote>
</div><div class="column" style="width:3%;">
<p>+</p>
</div><div class="column" style="width:30%;">
<blockquote>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>Sales</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S1</td>
<td>1000</td>
</tr>
<tr class="even">
<td>S2</td>
<td>1500</td>
</tr>
<tr class="odd">
<td>S3</td>
<td>…</td>
</tr>
</tbody>
</table>
</blockquote>
</div><div class="column" style="width:3%;">
<p>=</p>
</div><div class="column" style="width:30%;">
<blockquote>
<table class="caption-top">
<thead>
<tr class="header">
<th><code>StoreId</code></th>
<th><code>Type</code></th>
<th><code>Sales</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S1</td>
<td>grocery</td>
<td>1000</td>
</tr>
<tr class="even">
<td>S2</td>
<td>supermarket</td>
<td>1500</td>
</tr>
<tr class="odd">
<td>S3</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
</blockquote>
</div></div>
</section>

<section id="data-integration" class="title-slide slide level1 center">
<h1>Data integration</h1>
<p><strong>Data integration</strong> combines data residing in different sources and provides users with a unified view of them.</p>
<p><em>Primary key-based integration</em> combines multiple sources based on matching unique identifiers (primary keys).</p>
<ul>
<li>This method works when both datasets have a well-defined and consistent schema with common key fields.</li>
</ul>
<p><em>Semantic integration</em> focuses on understanding the meaning of the data from different sources to combine it effectively.</p>
<ul>
<li>The goal is to merge data that may use different names, terminologies, or structures to describe the same concepts.</li>
<li>Data is integrated based on semantic meaning rather than structural similarities.</li>
<li>It involves the use of ontologies or data dictionaries to map similar concepts across datasets, ensuring consistency.</li>
<li>It requires understanding the context, meaning, and relationships within the data.
<ul>
<li>For instance, spatial data can be easily integrated into maps</li>
</ul></li>
</ul>
</section>

<section id="semantic-integration-vs-primary-key-based-integration" class="title-slide slide level1 center">
<h1>Semantic Integration vs Primary Key-based Integration</h1>
<table class="caption-top">
<colgroup>
<col style="width: 10%">
<col style="width: 46%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Aspect</strong></th>
<th><strong>Semantic Integration</strong></th>
<th><strong>Primary Key-based Integration</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>Approach</em></td>
<td>Based on meaning and understanding of the data.</td>
<td>Based on matching unique keys.</td>
</tr>
<tr class="even">
<td><em>Suitability</em></td>
<td>Data with heterogeneous terminologies or structures.</td>
<td>Datasets have common, well-defined keys.</td>
</tr>
<tr class="odd">
<td><em>Complexity</em></td>
<td>Complex to interpret and align meanings.</td>
<td>Simpler, relies on exact key matches.</td>
</tr>
<tr class="even">
<td><em>Flexibility</em></td>
<td>Integrate data with different schemas/representations.</td>
<td>Less flexible, requires shared primary key fields.</td>
</tr>
<tr class="odd">
<td><em>Challenges</em></td>
<td>Requires mapping of concepts and domain semantics.</td>
<td>Limited to datasets that share a key.</td>
</tr>
</tbody>
</table>
</section>

<section id="format-data" class="title-slide slide level1 center">
<h1>Format Data</h1>
<p>In some cases, the data analyst will <em>change the format (structure) of the data</em>.</p>
<ul>
<li>Sometimes these changes are needed to make the data suitable for a specific modeling tool.</li>
<li>In other instances, the changes are needed to pose the necessary data mining questions.</li>
</ul>

<img data-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*YejjU_69ffDyrC0z-X9jYQ.jpeg" class="r-stretch quarto-figure-center"><p class="caption">5V’s of Big Data</p><p>Examples:</p>
<ul>
<li>Simple: removing illegal characters from strings or trimming them to a maximum length</li>
<li>More complex: reorganization of the information (e.g., <em>from normalized to flat tables</em>)</li>
</ul>
</section>

<section id="problem-how-do-we-concatenate-pre-processing-transformations" class="title-slide slide level1 center">
<h1><img src="./img/cs.svg" class="title-icon"> <strong>Problem</strong>: how do we concatenate pre-processing transformations?</h1>

</section>

<section id="sequences-of-transformations" class="title-slide slide level1 center">
<h1>Sequences of transformations</h1>
<p>Things are even more complex when applying sequences of transformations.</p>
<ul>
<li>E.g., normalization should be applied before rebalancing since rebalancing can alter average and standard deviations</li>
<li>E.g., applying feature engineering before/after rebalancing produces different results depending on the dataset and algorithm</li>
</ul>

<img data-src="https://user-images.githubusercontent.com/18005592/232754117-8a84fde5-bce2-41b1-a003-7dfa0b63f980.png" class="r-stretch quarto-figure-center"><p class="caption">image</p><p>More an art than a science</p>
<ul>
<li>… At least for now</li>
</ul>
</section>

<section id="final-considerations" class="title-slide slide level1 center">
<h1>Final considerations</h1>

</section>

<section id="overlapping-with-business-intelligence-and-data-warehousing" class="title-slide slide level1 center">
<h1>Overlapping with Business Intelligence and Data Warehousing</h1>
<p><em>ETL (Extract, Transform, Load)</em> is one of the most widely used data integration techniques in data warehousing.</p>
<ul>
<li><em>Extract</em>: Pull data from multiple sources (e.g., databases, APIs, flat files).</li>
<li><em>Transform</em>: Clean, standardize, and transform the data into the desired format.</li>
<li><em>Load</em>: Load the transformed data into a target database or data warehouse.</li>
</ul>
<p><em>ELT (Extract, Load, Transform)</em> loads data into a storage system (like a data lake) and then transforms within the storage system.</p>
</section>

<section id="overlapping-with-big-data-and-cloud-platforms" class="title-slide slide level1 center">
<h1>Overlapping with Big Data and Cloud Platforms</h1>
<ul>
<li>Data <em>profiling</em> to get metadata summarizing our dataset</li>
<li>Data <em>provenance</em> to track all the transformations that we apply to our dataset</li>
</ul>
</section>

<section id="wooclap" class="title-slide slide level1 center">
<h1>Wooclap</h1>
<iframe allowfullscreen="" frameborder="0" height="100%" mozallowfullscreen="" src="https://app.wooclap.com/ABXTPL/questionnaires/670e89672a9adb2c13ca5b23" style="min-height: 550px; min-width: 300px" width="100%">
</iframe>
</section>

<section id="references" class="title-slide slide level1 smaller scrollable">
<h1>References</h1>

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-chan2022mitigating" class="csl-entry" role="listitem">
Chan, Jireh Yi-Le, Steven Mun Hong Leow, Khean Thye Bea, Wai Khuen Cheng, Seuk Wai Phoong, Zeng-Wei Hong, and Yen-Lin Chen. 2022. <span>“Mitigating the Multicollinearity Problem and Its Machine Learning Approach: A Review.”</span> <em>Mathematics</em> 10 (8): 1283.
</div>
<div id="ref-hu2009overcoming" class="csl-entry" role="listitem">
Hu, Nan, Jie Zhang, and Paul A Pavlou. 2009. <span>“Overcoming the j-Shaped Distribution of Product Reviews.”</span> <em>Communications of the ACM</em> 52 (10): 144–47.
</div>
<div id="ref-jorion2000risk" class="csl-entry" role="listitem">
Jorion, Philippe. 2000. <span>“Risk Management Lessons from Long-Term Capital Management.”</span> <em>European Financial Management</em> 6 (3): 277–300.
</div>
<div id="ref-katrutsa2017comprehensive" class="csl-entry" role="listitem">
Katrutsa, Alexandr, and Vadim Strijov. 2017. <span>“Comprehensive Study of Feature Selection Methods to Solve Multicollinearity Problem According to Evaluation Criteria.”</span> <em>Expert Systems with Applications</em> 76: 1–11.
</div>
<div id="ref-liu2008isolation" class="csl-entry" role="listitem">
Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. 2008. <span>“Isolation Forest.”</span> In <em>2008 Eighth Ieee International Conference on Data Mining</em>, 413–22. IEEE.
</div>
<div id="ref-shearer2000crisp" class="csl-entry" role="listitem">
Shearer, Colin. 2000. <span>“The CRISP-DM Model: The New Blueprint for Data Mining.”</span> <em>Journal of Data Warehousing</em> 5 (4): 13–22.
</div>
<div id="ref-taleb2008impact" class="csl-entry" role="listitem">
Taleb, Nassim Nicholas. 2008. <em>The Impact of the Highly Improbable</em>. Penguin Books Limited.
</div>
<div id="ref-wu2017can" class="csl-entry" role="listitem">
Wu, Huiping, and Shing-On Leung. 2017. <span>“Can Likert Scales Be Treated as Interval Scales?—a Simulation Study.”</span> <em>Journal of Social Service Research</em> 43 (4): 527–32.
</div>
</div>
</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p>Matteo Francia - Machine Learning and Data Mining (Module 2) - A.Y. 2025/26</p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="04-datapreparation_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="04-datapreparation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="04-datapreparation_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="04-datapreparation_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="04-datapreparation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="04-datapreparation_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="04-datapreparation_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="04-datapreparation_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="04-datapreparation_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="04-datapreparation_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': true,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1280,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>